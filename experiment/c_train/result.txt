实验：1w Wang271K数据，8:2拆分，20epoch。看验证集Correction情况

baseline: 纯BERT（chinese-roberta-wwm-ext）
执行情况：花费1小时
Correction Precision: 0.05530088044822416, Recall: 0.1420560747663286, F1-Score: 0.07961032798921384
Correction Precision: 0.17577108881454528, Recall: 0.26503548748594224, F1-Score: 0.21136515927320912
Correction Precision: 0.24929044465464373, Recall: 0.29556926528317523, F1-Score: 0.2704644593444641
Correction Precision: 0.2969277215414141, Recall: 0.3124065769805096, F1-Score: 0.30447054488859465
Correction Precision: 0.3403013182673558, Recall: 0.33718977421154367, F1-Score: 0.3387384004747338
Correction Precision: 0.3632075471697399, Recall: 0.34516249533052945, F1-Score: 0.3539551805000651
Correction Precision: 0.4058879392211954, Recall: 0.39869402985067187, F1-Score: 0.402258823029376
Correction Precision: 0.45856052344593623, Recall: 0.4722066254912086, F1-Score: 0.4652835403022346
Correction Precision: 0.49203570803422153, Recall: 0.5248319641522544, F1-Score: 0.5079049592980612
Correction Precision: 0.5215438363000297, Recall: 0.5862004487657841, F1-Score: 0.5519852094672721
Correction Precision: 0.5429462966003378, Recall: 0.618406285072836, F1-Score: 0.578224748080934
Correction Precision: 0.5549999999999105, Recall: 0.6428171118997491, F1-Score: 0.5956894308191255
Correction Precision: 0.5656014132004873, Recall: 0.6583177570092227, F1-Score: 0.6084477839028679
Correction Precision: 0.5699383594119535, Recall: 0.6741446999437887, F1-Score: 0.617677286245437
Correction Precision: 0.5773519711008988, Recall: 0.6862049654656176, F1-Score: 0.627089729971021
Correction Precision: 0.5831769856159188, Recall: 0.6964152352500567, F1-Score: 0.6347855679176947
Correction Precision: 0.5861315008588808, Recall: 0.7012331838563711, F1-Score: 0.6385367923579929
Correction Precision: 0.5908596300325523, Recall: 0.7100691201194265, F1-Score: 0.6450025448969279
Correction Precision: 0.5956122607747634, Recall: 0.7147124719938919, F1-Score: 0.6497496388148921
Correction Precision: 0.6004063134863884, Recall: 0.71853375724692, F1-Score: 0.6541801459367275

模型：MultiModalBert，随便弄的，glyph和pinyin信息大概都是75%。
执行情况：花费2个多小时
Correction Precision: 0.0787985865724312, Recall: 0.16484938089074758, F1-Score: 0.10662841388307617
Correction Precision: 0.27237354085598064, Recall: 0.2715684463328521, F1-Score: 0.2719703972797843
Correction Precision: 0.40982763056331106, Recall: 0.2948362021098843, F1-Score: 0.34294940747879543
Correction Precision: 0.5084544645504276, Recall: 0.31693786982242655, F1-Score: 0.39047727484292544
Correction Precision: 0.5908529048205838, Recall: 0.3529628945910369, F1-Score: 0.4419276546801662
Correction Precision: 0.6726984126981992, Recall: 0.39153732446408135, F1-Score: 0.4949778084579314
Correction Precision: 0.7430615164518618, Recall: 0.47888622533644126, F1-Score: 0.5824175819408212
Correction Precision: 0.7909482758618559, Recall: 0.5426987060997148, F1-Score: 0.6437184823060457
Correction Precision: 0.8163057324838685, Recall: 0.5930038867294849, F1-Score: 0.6869639789292151
Correction Precision: 0.8420792079205837, Recall: 0.6290680473371617, F1-Score: 0.7201524127194746
Correction Precision: 0.8498334126604354, Recall: 0.6611738566930825, F1-Score: 0.7437259184912905
Correction Precision: 0.8599203560550808, Recall: 0.6779316712833466, F1-Score: 0.7581577855456724
Correction Precision: 0.8661710037172707, Recall: 0.6902425476762284, F1-Score: 0.768263781062129
Correction Precision: 0.8772663877264347, Recall: 0.6975970425137342, F1-Score: 0.7771828660631845
Correction Precision: 0.8764786169242773, Recall: 0.7119364375460621, F1-Score: 0.7856851544807353
Correction Precision: 0.8798098687186781, Recall: 0.7183515061909594, F1-Score: 0.790924813829899
Correction Precision: 0.886018581463656, Recall: 0.7231366746808353, F1-Score: 0.7963340117249199
Correction Precision: 0.8848728246316633, Recall: 0.7330868761551325, F1-Score: 0.8018600884650147
Correction Precision: 0.8909984364527828, Recall: 0.7376109467454257, F1-Score: 0.8070814360242516
Correction Precision: 0.8914067758580005, Recall: 0.7350601295095772, F1-Score: 0.8057189206157932


模型：MultiModalBert，和上面模型一样，但0.7*错字loss+0.3*复制字loss，相当于把侧重点放在改错字上
执行情况：召回率高，但精准率低。这是正常，因为模型不擅长复制原有字，而更偏向于改错，而且精准率还在不断增高。一共跑了30个epoch，这个方法有潜力。
Correction Precision: 0.02612711333750742, Recall: 0.30844575863975077, F1-Score: 0.048173644340866836
Correction Precision: 0.07302671033814474, Recall: 0.5848882320338842, F1-Score: 0.12984190108834653
Correction Precision: 0.11245818114851869, Recall: 0.678141773088899, F1-Score: 0.19292333589750169
Correction Precision: 0.14544098452358212, Recall: 0.7211538461537127, F1-Score: 0.24206312233493993
Correction Precision: 0.18556654061687614, Recall: 0.7518921912496304, F1-Score: 0.29766863960059764
Correction Precision: 0.22424209437760712, Recall: 0.7612712490759864, F1-Score: 0.3464368295832744
Correction Precision: 0.2640586797065849, Recall: 0.7766918679696152, F1-Score: 0.3941237013149178
Correction Precision: 0.2983320430712718, Recall: 0.7835489833639956, F1-Score: 0.4321321164260555
Correction Precision: 0.3451916829109531, Recall: 0.7866000370163266, F1-Score: 0.47981936169849404
Correction Precision: 0.37759663423609624, Recall: 0.7965976331359473, F1-Score: 0.5123387044455715
Correction Precision: 0.40329989687818474, Recall: 0.7965191631177936, F1-Score: 0.5354742340569981
Correction Precision: 0.44594177553745024, Recall: 0.8005540166203508, F1-Score: 0.5728065534515958
Correction Precision: 0.4702497285558664, Recall: 0.8018885391592664, F1-Score: 0.5928410097316956
Correction Precision: 0.49418604651157155, Recall: 0.8012939001846947, F1-Score: 0.6113383157034463
Correction Precision: 0.5258631132646243, Recall: 0.8021064301550623, F1-Score: 0.6352527982337695
Correction Precision: 0.5544352265474722, Recall: 0.8028090925890218, F1-Score: 0.6558961190999155
Correction Precision: 0.577132245655846, Recall: 0.804697614203661, F1-Score: 0.6721767336398277
Correction Precision: 0.6048699501936075, Recall: 0.8081330868760058, F1-Score: 0.6918816263499268
Correction Precision: 0.6271601382487576, Recall: 0.8052884615383126, F1-Score: 0.7051489632381874
Correction Precision: 0.6476984954564803, Recall: 0.804440333024828, F1-Score: 0.7176101661998809
Correction Precision: 0.6721109399074465, Recall: 0.8056889545620971, F1-Score: 0.7328629027297684
Correction Precision: 0.690891780387069, Recall: 0.8046571798187386, F1-Score: 0.7434474510523266
Correction Precision: 0.7055690072638086, Recall: 0.8072022160663329, F1-Score: 0.7529715757295172
Correction Precision: 0.7265611990007116, Recall: 0.8057248384116702, F1-Score: 0.7640980730563671
Correction Precision: 0.7463867859599541, Recall: 0.8025901942644212, F1-Score: 0.7734688414368668
Correction Precision: 0.7512454904654267, Recall: 0.8075715604799986, F1-Score: 0.7783908859369686
Correction Precision: 0.7648092810685946, Recall: 0.8041027536498236, F1-Score: 0.7839639634641362
Correction Precision: 0.7801075268815806, Recall: 0.8038781163433418, F1-Score: 0.7918144606186588
Correction Precision: 0.7958471150310922, Recall: 0.8001108442636615, F1-Score: 0.7979732837006855
Correction Precision: 0.8042632066726961, Recall: 0.8024782689105229, F1-Score: 0.8033697458431216

模型：MultiModalBert，将PinyinEmbeddings换成了手动Embedding（就是a=1,b=2这样），这样效果居然是最好的，中间不需要学习，而且在CSC任务上也表现更好。参数少了反而结果好了
执行情况：
Correction Precision: 0.07042019598072825, Recall: 0.15619819487932285, F1-Score: 0.09707515268249438
Correction Precision: 0.2600668190609706, Recall: 0.27302935203982404, F1-Score: 0.2663904894137923
Correction Precision: 0.40020237794070324, Recall: 0.2916666666666129, F1-Score: 0.3374213496242088
Correction Precision: 0.500737245650103, Recall: 0.31270718232038436, F1-Score: 0.3849903634309478
Correction Precision: 0.5763734518893057, Recall: 0.33468559837722023, F1-Score: 0.423471768083852
Correction Precision: 0.6475384132955009, Recall: 0.3806451612902524, F1-Score: 0.47945205432809934
Correction Precision: 0.7162503508277529, Recall: 0.47032805012892176, F1-Score: 0.5678050946376038
Correction Precision: 0.7764768493877657, Recall: 0.5375829034634234, F1-Score: 0.6353146087042816
Correction Precision: 0.8111111111109063, Recall: 0.5920737327187848, F1-Score: 0.6844965365392087
Correction Precision: 0.824282665401749, Recall: 0.642039157739076, F1-Score: 0.7218357382678269
Correction Precision: 0.8403361344535852, Recall: 0.6628613515005224, F1-Score: 0.7411219758320872
Correction Precision: 0.8546049555865229, Recall: 0.6734205194325523, F1-Score: 0.7532708349864256
Correction Precision: 0.8576642335764466, Recall: 0.6932153392329105, F1-Score: 0.7667210435511423
Correction Precision: 0.855849889624535, Recall: 0.7138648499354237, F1-Score: 0.7784358995139736
Correction Precision: 0.8692152917503086, Recall: 0.7172108467071172, F1-Score: 0.7859308666966691
Correction Precision: 0.8764019739791663, Recall: 0.7189915347808761, F1-Score: 0.789931257086592
Correction Precision: 0.8738079396760092, Recall: 0.7266691257837096, F1-Score: 0.7934749768477375
Correction Precision: 0.8858744394616846, Recall: 0.7278924097272056, F1-Score: 0.7991504849415232
Correction Precision: 0.8840035351301625, Recall: 0.7375115207371912, F1-Score: 0.8041402869122198
Correction Precision: 0.8906284454242798, Recall: 0.7446533923302461, F1-Score: 0.8111256145254097

模型：MultiModalBert，Manual Pinyin Embedding，Dense Glyph Embeddings, 但是Dense Glyph Embeddings网络是没有经过训练的。
执行情况：最终效果并不好，看来还是需要对Glyph Embedding层进行预训练才行。这么说网络在做预测时确实使用了Glyph信息
Correction Precision: 0.05667541074140158, Recall: 0.15122490329708024, F1-Score: 0.082450413864013
Correction Precision: 0.18047263681589795, Recall: 0.2667769810626463, F1-Score: 0.21529787028051212
Correction Precision: 0.2529170608640408, Recall: 0.2947445791987698, F1-Score: 0.27223353650219145
Correction Precision: 0.3028283192217073, Recall: 0.30929162833480967, F1-Score: 0.30602585058319687
Correction Precision: 0.34616843702573097, Recall: 0.3354162837713039, F1-Score: 0.34070755107291617
Correction Precision: 0.37673509286405144, Recall: 0.35494566218449897, F1-Score: 0.365515932732544
Correction Precision: 0.4165238911097627, Recall: 0.4028723991897619, F1-Score: 0.4095844248089352
Correction Precision: 0.4704508712052325, Recall: 0.48152233866510724, F1-Score: 0.47592222374130405
Correction Precision: 0.5097283085012253, Recall: 0.534362366776822, F1-Score: 0.5217547317151303
Correction Precision: 0.5363345135726628, Recall: 0.584742647058716, F1-Score: 0.5594934477463706
Correction Precision: 0.5559788009273673, Recall: 0.6173225450532148, F1-Score: 0.5850470542241647
Correction Precision: 0.570827943078821, Recall: 0.6502118253820869, F1-Score: 0.6079393777849673
Correction Precision: 0.5753577106517368, Recall: 0.665379665379543, F1-Score: 0.6171029068262543
Correction Precision: 0.5855086220533798, Recall: 0.6800808526275853, F1-Score: 0.6292612423829025
Correction Precision: 0.5900630914825568, Recall: 0.6883164673411797, F1-Score: 0.6354140122416991
Correction Precision: 0.5927204267335122, Recall: 0.694357654842732, F1-Score: 0.6395260257408428
Correction Precision: 0.6005391690452584, Recall: 0.6975501934056552, F1-Score: 0.6454196842065597
Correction Precision: 0.6011288805267166, Recall: 0.7059473393480563, F1-Score: 0.649335252276414
Correction Precision: 0.6064930991216112, Recall: 0.7109762824047231, F1-Score: 0.6545916203240241
Correction Precision: 0.6099235450147277, Recall: 0.7183020948179495, F1-Score: 0.6596911648057237



模型：MultiModalBert，最后的cls层使用了3层的前馈神经网络。不更新bert的权重，只更新cls的权重
执行情况：学习速度偏慢，但是能学会，到第20次的时候还在增长。到40次，f1到48时就出现瓶颈了
Correction Precision: 0.008080127935358807, Recall: 0.07181597157283115, F1-Score: 0.014525921400909513
Correction Precision: 0.02605912257578512, Recall: 0.12936997569636766, F1-Score: 0.04338014014216185
Correction Precision: 0.06378037160552263, Recall: 0.20037418147797934, F1-Score: 0.09676107837064266
Correction Precision: 0.11207194742302731, Recall: 0.24251497005983483, F1-Score: 0.15330021248460957
Correction Precision: 0.17475394827189575, Recall: 0.2859550561797217, F1-Score: 0.2169342231404072
Correction Precision: 0.24415903811992837, Recall: 0.334143605085951, F1-Score: 0.2821504692365423
Correction Precision: 0.3196187450356919, Recall: 0.37642656688486875, F1-Score: 0.345704466857223
Correction Precision: 0.38469694292270984, Recall: 0.4072965388212521, F1-Score: 0.3956742997547881
Correction Precision: 0.4555619818776835, Recall: 0.44242651188907645, F1-Score: 0.448898175791815
Correction Precision: 0.5116422831237355, Recall: 0.4644594089037664, F1-Score: 0.48691048092081984
Correction Precision: 0.5555317783007584, Recall: 0.48568755846576506, F1-Score: 0.5182671186876124
Correction Precision: 0.5985742927153934, Recall: 0.5026187803964641, F1-Score: 0.5464158612220381
Correction Precision: 0.639269406392548, Recall: 0.5242463958059307, F1-Score: 0.5760724200376349
Correction Precision: 0.6728164409153029, Recall: 0.5395131086141312, F1-Score: 0.5988360003373486
Correction Precision: 0.689922480619993, Recall: 0.5499999999998969, F1-Score: 0.6120662702158364
Correction Precision: 0.7267139479903718, Recall: 0.5752245508980959, F1-Score: 0.6421558382365287
Correction Precision: 0.749217809867449, Recall: 0.5823045267488622, F1-Score: 0.6552994416717016
Correction Precision: 0.765828902851434, Recall: 0.5931124836233215, F1-Score: 0.6684948840137894
Correction Precision: 0.781679764243423, Recall: 0.5957327344187543, F1-Score: 0.6761550712030736
Correction Precision: 0.794430992735885, Recall: 0.613615111277237, F1-Score: 0.6924132104398286
再学20次：
Correction Precision: 0.8120740019472219, Recall: 0.6239012530389706, F1-Score: 0.7056583813170033
Correction Precision: 0.81771720613268, Recall: 0.6281547952887215, F1-Score: 0.7105096209929016
Correction Precision: 0.8236580034003342, Recall: 0.6344246959774303, F1-Score: 0.7167617834862875
Correction Precision: 0.836779372415267, Recall: 0.6437125748501789, F1-Score: 0.7276573236754563
Correction Precision: 0.840309702395151, Recall: 0.6503745318350841, F1-Score: 0.733241844752341
Correction Precision: 0.8455971049455137, Recall: 0.6553851907253823, F1-Score: 0.7384388491865846
Correction Precision: 0.853188929000998, Recall: 0.6632366697847215, F1-Score: 0.7463157889813723
Correction Precision: 0.8585907335905263, Recall: 0.6656688493918305, F1-Score: 0.7499209606207189
Correction Precision: 0.860687022900558, Recall: 0.6755289271670706, F1-Score: 0.7569495431974435
Correction Precision: 0.861961722487832, Recall: 0.6739618406283812, F1-Score: 0.7564560146238554
Correction Precision: 0.8667944417822551, Recall: 0.6768942937323336, F1-Score: 0.7601638822682009
Correction Precision: 0.8672714251609291, Recall: 0.6795735129067191, F1-Score: 0.7620346088412346
Correction Precision: 0.8678885979526617, Recall: 0.6826436996815797, F1-Score: 0.7642003767863729
Correction Precision: 0.8730385164049279, Recall: 0.6876404494380735, F1-Score: 0.7693274665087804
Correction Precision: 0.8734356552536308, Recall: 0.6926966292133534, F1-Score: 0.7726370752245133
Correction Precision: 0.8795352146073322, Recall: 0.6940494011974748, F1-Score: 0.7758602651693725
Correction Precision: 0.879461120302321, Recall: 0.6960344182565102, F1-Score: 0.7770700632008825
Correction Precision: 0.8794359576966205, Recall: 0.7003556054649633, F1-Score: 0.779745779877173
Correction Precision: 0.8804143126174951, Recall: 0.6999812839227587, F1-Score: 0.7798978203800905
Correction Precision: 0.8796816479398689, Recall: 0.7028240134653632, F1-Score: 0.7813702043090763
Correction Precision: 0.8834427767352524, Recall: 0.7045072002991014, F1-Score: 0.7838934549218551
Correction Precision: 0.8850198644541049, Recall: 0.7083800972688536, F1-Score: 0.7869090904150721

模型：MultiModalBert，最后的cls层使用了2层的Transformer。不更新bert的权重，只更新cls的权重
执行情况：比3层Dense层好，训练到40个epoch后效果和之前的差不都，虽然还在涨，但感觉涨不太动了。
Correction Precision: 0.0038887773172912956, Recall: 0.039335180055394395, F1-Score: 0.007077822658077036
Correction Precision: 0.024105629975382074, Recall: 0.13921713441651787, F1-Score: 0.041095516848798676
Correction Precision: 0.057618497109823925, Recall: 0.22993172171983206, F1-Score: 0.09214613190850107
Correction Precision: 0.09968827930173942, Recall: 0.29512735326683365, F1-Score: 0.14903532444308504
Correction Precision: 0.15292973498661536, Recall: 0.3481549815497512, F1-Score: 0.21251196533971026
Correction Precision: 0.2155562255049527, Recall: 0.3959025470652647, F1-Score: 0.2791333198635847
Correction Precision: 0.2769830949284458, Recall: 0.43260709010331744, F1-Score: 0.3377297292537307
Correction Precision: 0.3364661654134886, Recall: 0.4632162661736666, F1-Score: 0.3897962353190023
Correction Precision: 0.389415950192649, Recall: 0.4844182186980482, F1-Score: 0.43175281403455945
Correction Precision: 0.4398774983880658, Recall: 0.5035984498984123, F1-Score: 0.46958616486398264
Correction Precision: 0.48995708154498024, Recall: 0.527541589648701, F1-Score: 0.5080551841912904
Correction Precision: 0.5364394488758998, Recall: 0.5458402508761213, F1-Score: 0.5410990211695008
Correction Precision: 0.5791258477768313, Recall: 0.5675775480058036, F1-Score: 0.5732935466838304
Correction Precision: 0.6156694601441014, Recall: 0.5834872552640222, F1-Score: 0.599146514436235
Correction Precision: 0.6457336523124709, Recall: 0.5981532779315608, F1-Score: 0.6210334574624569
Correction Precision: 0.6777755060313477, Recall: 0.6121883656508564, F1-Score: 0.6433145735357106
Correction Precision: 0.6961712638944635, Recall: 0.6239852398522834, F1-Score: 0.6581046891296991
Correction Precision: 0.7156249999998509, Recall: 0.6338807898135017, F1-Score: 0.6722771303364224
Correction Precision: 0.7300420168065692, Recall: 0.6417359187441104, F1-Score: 0.6830466825486207
Correction Precision: 0.7457088366177694, Recall: 0.6499815293681843, F1-Score: 0.6945623206311995
Correction Precision: 0.7645429362879257, Recall: 0.6628486975797777, F1-Score: 0.7100732233298266
Correction Precision: 0.7773479475605464, Recall: 0.6679593721143734, F1-Score: 0.7185141035948945
Correction Precision: 0.7924691625187638, Recall: 0.6757704373499398, F1-Score: 0.7294820712161606
Correction Precision: 0.7993506493504763, Recall: 0.6811139800810252, F1-Score: 0.7355108539145143
Correction Precision: 0.8087277464175404, Recall: 0.6875230712438746, F1-Score: 0.7432162804289109
Correction Precision: 0.8178058336959473, Recall: 0.6931734317341894, F1-Score: 0.7503495101882929
Correction Precision: 0.8325285338013976, Recall: 0.6999446392322015, F1-Score: 0.7605012526364222
Correction Precision: 0.8353792196403254, Recall: 0.703915773919338, F1-Score: 0.7640336803374954
Correction Precision: 0.8404022737208481, Recall: 0.7096178696694277, F1-Score: 0.7694925427923656
Correction Precision: 0.8480349344976313, Recall: 0.7170020306441356, F1-Score: 0.7770331094363292
Correction Precision: 0.8517626450622177, Recall: 0.7177121771216387, F1-Score: 0.7790127160349823
Correction Precision: 0.8564530289725951, Recall: 0.7205909510617321, F1-Score: 0.7826697417560863
Correction Precision: 0.8593030900721325, Recall: 0.722631772944946, F1-Score: 0.7850635694304944
Correction Precision: 0.8635164835162936, Recall: 0.7245067305917896, F1-Score: 0.7879274034944482
Correction Precision: 0.8666959000217349, Recall: 0.7290667650312193, F1-Score: 0.7919463082283926
Correction Precision: 0.8686403508770024, Recall: 0.7310815799186542, F1-Score: 0.793946682205482
Correction Precision: 0.8735404274067253, Recall: 0.7318198597267014, F1-Score: 0.7964246253950694
Correction Precision: 0.8739864124477593, Recall: 0.736608792020551, F1-Score: 0.7994387085342481
Correction Precision: 0.8758741258739344, Recall: 0.7404396822463069, F1-Score: 0.8024827305074542
Correction Precision: 0.8809732573430773, Recall: 0.7411027106766105, F1-Score: 0.8050075107704568
Correction Precision: 0.8807017543857717, Recall: 0.7420546932740683, F1-Score: 0.8054552742728425
Correction Precision: 0.8846238694019667, Recall: 0.7406723309935831, F1-Score: 0.8062732477193024

模型：MDCSpell
执行情况：效果确实比只用bert好很多，到15个epoch时，基本已经可以完全拟合训练数据了。
Correction Precision: 0.9830810329470244, Recall: 0.41495959406118865, F1-Score: 0.5835866257222314
Correction Precision: 0.9846205282510917, Recall: 0.5529478032293366, F1-Score: 0.7081880481144472
Correction Precision: 0.9842073897494088, Recall: 0.619932432432316, F1-Score: 0.7607093500553181
Correction Precision: 0.9785360484312112, Recall: 0.6676680435597694, F1-Score: 0.7937499995176529
Correction Precision: 0.9826203208553522, Recall: 0.6896228185399343, F1-Score: 0.8104531917083564
Correction Precision: 0.9818229031417861, Recall: 0.7100469483566741, F1-Score: 0.8241063639416675
Correction Precision: 0.9806240563661347, Recall: 0.7308702175542514, F1-Score: 0.8375241774601772
Correction Precision: 0.9815645241651765, Recall: 0.7396283086163432, F1-Score: 0.8435927625971622
Correction Precision: 0.9824518042508694, Recall: 0.7463387157339942, F1-Score: 0.8482714463721418
Correction Precision: 0.9801373222165326, Recall: 0.7506103286383566, F1-Score: 0.8501542056130476
Epoch 10 Training: 100% 250/250 [01:41<00:00,  2.47it/s, loss=0.0211, c_precision=0.999, c_recall=0.979, c_f1_score=0.989]
Correction Precision: 0.9824347401802921, Recall: 0.7568126292048943, F1-Score: 0.8549893837969799
Epoch 11 Training: 100% 250/250 [02:41<00:00,  1.55it/s, loss=0.014, c_precision=0.999, c_recall=0.983, c_f1_score=0.991]
Correction Precision: 0.9841540711845479, Recall: 0.7579797221177698, F1-Score: 0.856385234977062
Epoch 12 Training: 100% 250/250 [02:42<00:00,  1.54it/s, loss=0.0131, c_precision=0.999, c_recall=0.991, c_f1_score=0.995]
Correction Precision: 0.9851653696495658, Recall: 0.76032282282268, F1-Score: 0.8582627113725213
Epoch 13 Training: 100% 250/250 [02:44<00:00,  1.52it/s, loss=0.0154, c_precision=0.999, c_recall=0.995, c_f1_score=0.997]
Correction Precision: 0.9823330106483584, Recall: 0.7621104018023352, F1-Score: 0.8583209976046362
Epoch 14 Training: 100% 250/250 [02:44<00:00,  1.52it/s, loss=0.0102, c_precision=0.999, c_recall=0.997, c_f1_score=0.998]
Correction Precision: 0.9842843326883499, Recall: 0.7639331957213803, F1-Score: 0.8602218695553052
Epoch 15 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.0114, c_precision=0.999, c_recall=0.997, c_f1_score=0.998]
Correction Precision: 0.983075435202857, Recall: 0.7635680751172275, F1-Score: 0.8595285905657428
Epoch 16 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.00963, c_precision=0.999, c_recall=0.998, c_f1_score=0.999]
Correction Precision: 0.9833574529664776, Recall: 0.7646286571641476, F1-Score: 0.8603080813814604
Epoch 17 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.01, c_precision=1, c_recall=0.999, c_f1_score=0.999]
Correction Precision: 0.9850313858037215, Recall: 0.7659095175519492, F1-Score: 0.8617594250013648
Epoch 18 Training: 100% 250/250 [02:46<00:00,  1.50it/s, loss=0.00942, c_precision=1, c_recall=0.999, c_f1_score=1]
Correction Precision: 0.9836499158449185, Recall: 0.7681186631617033, F1-Score: 0.8626251971879352
Epoch 19 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.0081, c_precision=1, c_recall=0.999, c_f1_score=0.999]
Correction Precision: 0.9847972972970596, Recall: 0.7663849765256776, F1-Score: 0.8619706405467762
Eval：
Detection Character-level Precision 0.8703703703671468, Recall 0.33428165007064825, F1_Score 0.48304213731642315
Correction Character-level Precision 0.8648648648615257, Recall 0.32369942196485013, F1_Score 0.47108307005481853
Detection Sentence-level Precision 0.7654320987339328, Recall 0.343807763394754, F1_Score 0.4744897916286509
Correction Sentence-level Precision 0.7283950616984199, Recall 0.32717190387565304, F1_Score 0.4515306079557672

模型：MDCSpellPlus，MDCSpell使用MultiModalBert作为Bert
执行情况：看起来是比单纯的使用Bert效果好，大概在第28个epoch完全拟合了训练集，最终效果没有原本的MDCSpell效果好
Correction Precision: 0.10402249134946971, Recall: 0.17998129092606546, F1-Score: 0.13184403434695938
Correction Precision: 0.33396494552336947, Recall: 0.2643419572552935, F1-Score: 0.2951025528763196
Correction Precision: 0.4976198032369097, Recall: 0.2933034044144606, F1-Score: 0.36907143651103624
Correction Precision: 0.5975895547369945, Recall: 0.3342696629212857, F1-Score: 0.42872583117186164
Correction Precision: 0.7018147684603562, Recall: 0.420273561926097, F1-Score: 0.5257236606118652
Correction Precision: 0.7784496976358498, Recall: 0.5310331895742488, F1-Score: 0.6313677400148663
Correction Precision: 0.8336870026522988, Recall: 0.5887973023603242, F1-Score: 0.6901624940249879
Correction Precision: 0.8389604080639205, Recall: 0.6465743167351841, F1-Score: 0.7303097574107581
Correction Precision: 0.8692345436700517, Recall: 0.663980509745003, F1-Score: 0.7528686777918437
Correction Precision: 0.8752422480618034, Recall: 0.6769720816937086, F1-Score: 0.7634442678651017
Correction Precision: 0.8778013682470209, Recall: 0.6968164794006185, F1-Score: 0.7769078186942497
Correction Precision: 0.8799628511723054, Recall: 0.7100037467214855, F1-Score: 0.7858994292619005
Correction Precision: 0.8836889194767414, Recall: 0.7210782478471132, F1-Score: 0.7941449330169791
Correction Precision: 0.880952380952183, Recall: 0.7345944933506771, F1-Score: 0.8011439071742492
Correction Precision: 0.8962808962806892, Recall: 0.7265917602994894, F1-Score: 0.8025648976333385
Correction Precision: 0.8918918918916893, Recall: 0.7364966241559008, F1-Score: 0.8067796605213365
Correction Precision: 0.8986995208759071, Recall: 0.7370883233531554, F1-Score: 0.8099105577446644
Correction Precision: 0.8981941309253051, Recall: 0.7455499344199464, F1-Score: 0.8147844778495938
Correction Precision: 0.9041002277902268, Recall: 0.742841100505195, F1-Score: 0.8155758753907864
Correction Precision: 0.9057414104880411, Recall: 0.7502340385694157, F1-Score: 0.8206861234161715
Correction Precision: 0.906618313689731, Recall: 0.7483629560335363, F1-Score: 0.8199241565198798
Correction Precision: 0.9018336314845925, Recall: 0.7560929883763111, F1-Score: 0.8225576172887252
Correction Precision: 0.9109311740888638, Recall: 0.7575757575756158, F1-Score: 0.8272058818569961
Correction Precision: 0.9015405224378429, Recall: 0.7561797752807572, F1-Score: 0.8224870144746514
Correction Precision: 0.9037647582978605, Recall: 0.7601648866402922, F1-Score: 0.8257683691351455
Correction Precision: 0.9168949771687404, Recall: 0.7530470654414488, F1-Score: 0.8269329759279931
Correction Precision: 0.9074280615657132, Recall: 0.7620831772197897, F1-Score: 0.8284288764000671
Correction Precision: 0.9117381489839927, Recall: 0.7560838637213111, F1-Score: 0.8266475639740999
Eval:
Detection Character-level Precision 0.44703143189703487, Recall 0.5462304409665061, F1_Score 0.49167733625211696
Correction Character-level Precision 0.39949431099823074, Recall 0.4976377952748069, F1_Score 0.4431977554660916
Detection Sentence-level Precision 0.34090909090355664, Recall 0.38817005544569, F1_Score 0.3630077737528508
Correction Sentence-level Precision 0.282467532462947, Recall 0.321626617369286, F1_Score 0.3007778688273925


模型：MacBert4Csc，还未使用源代码中的schedule和optimizer
执行情况：效果较差。可以看出schedule和optimizer确实很重要。没有这俩东西在14个epoch就early-stop了
Correction Precision: 0.6147368421051552, Recall: 0.650092764378358, F1-Score: 0.6319206487338203
Correction Precision: 0.6328730212191591, Recall: 0.6963127663515478, F1-Score: 0.663078958477643
Correction Precision: 0.6428099173552656, Recall: 0.719518963922161, F1-Score: 0.6790048008982366
Correction Precision: 0.6482501620219299, Recall: 0.7405145289652525, F1-Score: 0.6913174941025193
Correction Precision: 0.6527597402596342, Recall: 0.7447675495460743, F1-Score: 0.6957349246685837
Correction Precision: 0.6530777976286092, Recall: 0.7450435427087743, F1-Score: 0.6960360043488492
Correction Precision: 0.6545277190699634, Recall: 0.7455555555554174, F1-Score: 0.6970825031813279
Correction Precision: 0.6554379210778544, Recall: 0.7570872707058074, F1-Score: 0.7026051065439953
Correction Precision: 0.657434402332255, Recall: 0.7518058899794865, F1-Score: 0.7014602950175447
Correction Precision: 0.6601159793813369, Recall: 0.7586079229913442, F1-Score: 0.7059431519570685
Correction Precision: 0.658473479948147, Recall: 0.7537948907810526, F1-Score: 0.7029173135018112
Correction Precision: 0.6585562359006351, Recall: 0.7572725588288387, F1-Score: 0.7044729806278802
Correction Precision: 0.660209846650418, Recall: 0.757688032604528, F1-Score: 0.7055982053160944
Correction Precision: 0.6597738287559515, Recall: 0.7564363771067315, F1-Score: 0.704806281321174
Correction Precision: 0.6599001127757919, Recall: 0.758518518518378, F1-Score: 0.7057809937299167

模型：MacBert4Csc，使用了源码中的optimizer（AdamW），但还未使用schedule
执行情况：和不用这个optimizer效果差不多，看来optimizer还是要和schedule配合起来才行，属于联合超参数。
Correction Precision: 0.6342253281062494, Recall: 0.6903525046380908, F1-Score: 0.6610997596500208
Correction Precision: 0.6486311355903696, Recall: 0.7418936446172425, F1-Score: 0.6921348309628036
Correction Precision: 0.6541268039564367, Recall: 0.746345975948058, F1-Score: 0.6972001377668244
Correction Precision: 0.6555466879488179, Recall: 0.7601332593002479, F1-Score: 0.7039766878810791
Correction Precision: 0.6603803739810356, Recall: 0.7653269123910418, F1-Score: 0.7089910770592107
Correction Precision: 0.6593090211131383, Recall: 0.7637576431349334, F1-Score: 0.7077002312821239
Correction Precision: 0.6595471334509941, Recall: 0.7605555555554147, F1-Score: 0.7064591033125052
Correction Precision: 0.6608318612492916, Recall: 0.7624606262737145, F1-Score: 0.7080178935148156
Correction Precision: 0.6632049848217505, Recall: 0.7688460826077479, F1-Score: 0.7121290096244034
Correction Precision: 0.6634553628772216, Recall: 0.7649018881894177, F1-Score: 0.7105760958050654
Correction Precision: 0.6602717825738352, Recall: 0.7645316549424722, F1-Score: 0.7085871145407301
Correction Precision: 0.663052144793388, Recall: 0.7704280155640595, F1-Score: 0.7127185459545171
Correction Precision: 0.6616129032256997, Recall: 0.7610389610388197, F1-Score: 0.7078515957059437
Correction Precision: 0.6592948717947661, Recall: 0.7622753381506833, F1-Score: 0.7070550824276547
Correction Precision: 0.6607228915661588, Recall: 0.7609620721552708, F1-Score: 0.7073086839391652
Correction Precision: 0.6605356566633332, Recall: 0.7577271885987862, F1-Score: 0.7058012235346374
Epoch 16 Training: 100% 250/250 [02:35<00:00,  1.60it/s, loss=0.0109, c_precision=0.732, c_recall=0.997, c_f1_score=0.844]
Correction Precision: 0.6601675797614147, Recall: 0.7588442304128988, F1-Score: 0.706074967188638

模型：MacBert4Csc，完全模拟了MacBert4Csc的训练流程
执行情况：看起来效果并不好，有两种可能性。一是在大数据量上表现会好，二是在我的代码有问题
Correction Precision: 0.5704747774479654, Recall: 0.5706864564006362, F1-Score: 0.5705805967916766
Correction Precision: 0.6170952050033673, Recall: 0.6581434130071042, F1-Score: 0.6369586653302253
Correction Precision: 0.6379776800810554, Recall: 0.6980573543014434, F1-Score: 0.6666666661675599
Correction Precision: 0.6459011060506431, Recall: 0.7349620581157255, F1-Score: 0.6875595181583541
Correction Precision: 0.6516744863289675, Recall: 0.7460640859417028, F1-Score: 0.6956822102102774
Correction Precision: 0.6543369407202948, Recall: 0.7506021863997127, F1-Score: 0.6991715562851054
Correction Precision: 0.6557825317676281, Recall: 0.7549999999998601, F1-Score: 0.7019023839389535
Correction Precision: 0.6571337274323922, Recall: 0.762090050027652, F1-Score: 0.705730953517713
Correction Precision: 0.6591712174749985, Recall: 0.7601407668085274, F1-Score: 0.7060645156314411
Correction Precision: 0.6602564102563044, Recall: 0.7626804887077447, F1-Score: 0.7077821675148379
Correction Precision: 0.6600064453753367, Recall: 0.7582376897443986, F1-Score: 0.7057201924726422
Correction Precision: 0.6613603473226143, Recall: 0.762090050027652, F1-Score: 0.708161156527175
Correction Precision: 0.6618635926992216, Recall: 0.7658391997034519, F1-Score: 0.7100652691693109
Correction Precision: 0.66088353413644, Recall: 0.7619929616594254, F1-Score: 0.7078458357033665
Correction Precision: 0.6600543912972868, Recall: 0.7640740740739325, F1-Score: 0.7082653844480441
Correction Precision: 0.6623272259722497, Recall: 0.763006850583084, F1-Score: 0.7091112444477395
Epoch 16 Training: 100% 250/250 [02:33<00:00,  1.63it/s, loss=0.00943, c_precision=0.728, c_recall=0.997, c_f1_score=0.841]
Correction Precision: 0.6621248986211415, Recall: 0.7550869404364122, F1-Score: 0.7055569954401505

Evel情况：经过一次全量数据的训练后，在sighan上的character-level如下，效果还是想当好的。看来optimizer和scheduler非常重要啊。
The detection result is precision=0.7780979827089337, recall=0.7670454545454546 and F1=0.7725321888412018
The correction result is precision=0.924074074074074, recall=0.8414839797639123 and F1=0.880847308031774
Sentence Level: acc:0.7482, precision:0.8103, recall:0.6384, f1:0.7141, , total num: 1100

