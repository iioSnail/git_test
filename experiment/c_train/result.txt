实验：1w Wang271K数据，8:2拆分，20epoch。看验证集Correction情况

baseline: 纯BERT（chinese-roberta-wwm-ext）
执行情况：花费1小时
Correction Precision: 0.05530088044822416, Recall: 0.1420560747663286, F1-Score: 0.07961032798921384
Correction Precision: 0.17577108881454528, Recall: 0.26503548748594224, F1-Score: 0.21136515927320912
Correction Precision: 0.24929044465464373, Recall: 0.29556926528317523, F1-Score: 0.2704644593444641
Correction Precision: 0.2969277215414141, Recall: 0.3124065769805096, F1-Score: 0.30447054488859465
Correction Precision: 0.3403013182673558, Recall: 0.33718977421154367, F1-Score: 0.3387384004747338
Correction Precision: 0.3632075471697399, Recall: 0.34516249533052945, F1-Score: 0.3539551805000651
Correction Precision: 0.4058879392211954, Recall: 0.39869402985067187, F1-Score: 0.402258823029376
Correction Precision: 0.45856052344593623, Recall: 0.4722066254912086, F1-Score: 0.4652835403022346
Correction Precision: 0.49203570803422153, Recall: 0.5248319641522544, F1-Score: 0.5079049592980612
Correction Precision: 0.5215438363000297, Recall: 0.5862004487657841, F1-Score: 0.5519852094672721
Correction Precision: 0.5429462966003378, Recall: 0.618406285072836, F1-Score: 0.578224748080934
Correction Precision: 0.5549999999999105, Recall: 0.6428171118997491, F1-Score: 0.5956894308191255
Correction Precision: 0.5656014132004873, Recall: 0.6583177570092227, F1-Score: 0.6084477839028679
Correction Precision: 0.5699383594119535, Recall: 0.6741446999437887, F1-Score: 0.617677286245437
Correction Precision: 0.5773519711008988, Recall: 0.6862049654656176, F1-Score: 0.627089729971021
Correction Precision: 0.5831769856159188, Recall: 0.6964152352500567, F1-Score: 0.6347855679176947
Correction Precision: 0.5861315008588808, Recall: 0.7012331838563711, F1-Score: 0.6385367923579929
Correction Precision: 0.5908596300325523, Recall: 0.7100691201194265, F1-Score: 0.6450025448969279
Correction Precision: 0.5956122607747634, Recall: 0.7147124719938919, F1-Score: 0.6497496388148921
Correction Precision: 0.6004063134863884, Recall: 0.71853375724692, F1-Score: 0.6541801459367275


模型：MDCSpell
执行情况：效果确实比只用bert好很多，到15个epoch时，基本已经可以完全拟合训练数据了。
Correction Precision: 0.9830810329470244, Recall: 0.41495959406118865, F1-Score: 0.5835866257222314
Correction Precision: 0.9846205282510917, Recall: 0.5529478032293366, F1-Score: 0.7081880481144472
Correction Precision: 0.9842073897494088, Recall: 0.619932432432316, F1-Score: 0.7607093500553181
Correction Precision: 0.9785360484312112, Recall: 0.6676680435597694, F1-Score: 0.7937499995176529
Correction Precision: 0.9826203208553522, Recall: 0.6896228185399343, F1-Score: 0.8104531917083564
Correction Precision: 0.9818229031417861, Recall: 0.7100469483566741, F1-Score: 0.8241063639416675
Correction Precision: 0.9806240563661347, Recall: 0.7308702175542514, F1-Score: 0.8375241774601772
Correction Precision: 0.9815645241651765, Recall: 0.7396283086163432, F1-Score: 0.8435927625971622
Correction Precision: 0.9824518042508694, Recall: 0.7463387157339942, F1-Score: 0.8482714463721418
Correction Precision: 0.9801373222165326, Recall: 0.7506103286383566, F1-Score: 0.8501542056130476
Epoch 10 Training: 100% 250/250 [01:41<00:00,  2.47it/s, loss=0.0211, c_precision=0.999, c_recall=0.979, c_f1_score=0.989]
Correction Precision: 0.9824347401802921, Recall: 0.7568126292048943, F1-Score: 0.8549893837969799
Epoch 11 Training: 100% 250/250 [02:41<00:00,  1.55it/s, loss=0.014, c_precision=0.999, c_recall=0.983, c_f1_score=0.991]
Correction Precision: 0.9841540711845479, Recall: 0.7579797221177698, F1-Score: 0.856385234977062
Epoch 12 Training: 100% 250/250 [02:42<00:00,  1.54it/s, loss=0.0131, c_precision=0.999, c_recall=0.991, c_f1_score=0.995]
Correction Precision: 0.9851653696495658, Recall: 0.76032282282268, F1-Score: 0.8582627113725213
Epoch 13 Training: 100% 250/250 [02:44<00:00,  1.52it/s, loss=0.0154, c_precision=0.999, c_recall=0.995, c_f1_score=0.997]
Correction Precision: 0.9823330106483584, Recall: 0.7621104018023352, F1-Score: 0.8583209976046362
Epoch 14 Training: 100% 250/250 [02:44<00:00,  1.52it/s, loss=0.0102, c_precision=0.999, c_recall=0.997, c_f1_score=0.998]
Correction Precision: 0.9842843326883499, Recall: 0.7639331957213803, F1-Score: 0.8602218695553052
Epoch 15 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.0114, c_precision=0.999, c_recall=0.997, c_f1_score=0.998]
Correction Precision: 0.983075435202857, Recall: 0.7635680751172275, F1-Score: 0.8595285905657428
Epoch 16 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.00963, c_precision=0.999, c_recall=0.998, c_f1_score=0.999]
Correction Precision: 0.9833574529664776, Recall: 0.7646286571641476, F1-Score: 0.8603080813814604
Epoch 17 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.01, c_precision=1, c_recall=0.999, c_f1_score=0.999]
Correction Precision: 0.9850313858037215, Recall: 0.7659095175519492, F1-Score: 0.8617594250013648
Epoch 18 Training: 100% 250/250 [02:46<00:00,  1.50it/s, loss=0.00942, c_precision=1, c_recall=0.999, c_f1_score=1]
Correction Precision: 0.9836499158449185, Recall: 0.7681186631617033, F1-Score: 0.8626251971879352
Epoch 19 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.0081, c_precision=1, c_recall=0.999, c_f1_score=0.999]
Correction Precision: 0.9847972972970596, Recall: 0.7663849765256776, F1-Score: 0.8619706405467762
Eval：
Detection Character-level Precision 0.8703703703671468, Recall 0.33428165007064825, F1_Score 0.48304213731642315
Correction Character-level Precision 0.8648648648615257, Recall 0.32369942196485013, F1_Score 0.47108307005481853
Detection Sentence-level Precision 0.7654320987339328, Recall 0.343807763394754, F1_Score 0.4744897916286509
Correction Sentence-level Precision 0.7283950616984199, Recall 0.32717190387565304, F1_Score 0.4515306079557672

模型：MDCSpellPlus，MDCSpell使用MultiModalBert作为Bert
执行情况：看起来是比单纯的使用Bert效果好，大概在第28个epoch完全拟合了训练集，最终效果没有原本的MDCSpell效果好
Correction Precision: 0.10402249134946971, Recall: 0.17998129092606546, F1-Score: 0.13184403434695938
Correction Precision: 0.33396494552336947, Recall: 0.2643419572552935, F1-Score: 0.2951025528763196
Correction Precision: 0.4976198032369097, Recall: 0.2933034044144606, F1-Score: 0.36907143651103624
Correction Precision: 0.5975895547369945, Recall: 0.3342696629212857, F1-Score: 0.42872583117186164
Correction Precision: 0.7018147684603562, Recall: 0.420273561926097, F1-Score: 0.5257236606118652
Correction Precision: 0.7784496976358498, Recall: 0.5310331895742488, F1-Score: 0.6313677400148663
Correction Precision: 0.8336870026522988, Recall: 0.5887973023603242, F1-Score: 0.6901624940249879
Correction Precision: 0.8389604080639205, Recall: 0.6465743167351841, F1-Score: 0.7303097574107581
Correction Precision: 0.8692345436700517, Recall: 0.663980509745003, F1-Score: 0.7528686777918437
Correction Precision: 0.8752422480618034, Recall: 0.6769720816937086, F1-Score: 0.7634442678651017
Correction Precision: 0.8778013682470209, Recall: 0.6968164794006185, F1-Score: 0.7769078186942497
Correction Precision: 0.8799628511723054, Recall: 0.7100037467214855, F1-Score: 0.7858994292619005
Correction Precision: 0.8836889194767414, Recall: 0.7210782478471132, F1-Score: 0.7941449330169791
Correction Precision: 0.880952380952183, Recall: 0.7345944933506771, F1-Score: 0.8011439071742492
Correction Precision: 0.8962808962806892, Recall: 0.7265917602994894, F1-Score: 0.8025648976333385
Correction Precision: 0.8918918918916893, Recall: 0.7364966241559008, F1-Score: 0.8067796605213365
Correction Precision: 0.8986995208759071, Recall: 0.7370883233531554, F1-Score: 0.8099105577446644
Correction Precision: 0.8981941309253051, Recall: 0.7455499344199464, F1-Score: 0.8147844778495938
Correction Precision: 0.9041002277902268, Recall: 0.742841100505195, F1-Score: 0.8155758753907864
Correction Precision: 0.9057414104880411, Recall: 0.7502340385694157, F1-Score: 0.8206861234161715
Correction Precision: 0.906618313689731, Recall: 0.7483629560335363, F1-Score: 0.8199241565198798
Correction Precision: 0.9018336314845925, Recall: 0.7560929883763111, F1-Score: 0.8225576172887252
Correction Precision: 0.9109311740888638, Recall: 0.7575757575756158, F1-Score: 0.8272058818569961
Correction Precision: 0.9015405224378429, Recall: 0.7561797752807572, F1-Score: 0.8224870144746514
Correction Precision: 0.9037647582978605, Recall: 0.7601648866402922, F1-Score: 0.8257683691351455
Correction Precision: 0.9168949771687404, Recall: 0.7530470654414488, F1-Score: 0.8269329759279931
Correction Precision: 0.9074280615657132, Recall: 0.7620831772197897, F1-Score: 0.8284288764000671
Correction Precision: 0.9117381489839927, Recall: 0.7560838637213111, F1-Score: 0.8266475639740999
Eval:
Detection Character-level Precision 0.44703143189703487, Recall 0.5462304409665061, F1_Score 0.49167733625211696
Correction Character-level Precision 0.39949431099823074, Recall 0.4976377952748069, F1_Score 0.4431977554660916
Detection Sentence-level Precision 0.34090909090355664, Recall 0.38817005544569, F1_Score 0.3630077737528508
Correction Sentence-level Precision 0.282467532462947, Recall 0.321626617369286, F1_Score 0.3007778688273925


模型：MacBert4Csc，还未使用源代码中的schedule和optimizer
执行情况：效果较差。可以看出schedule和optimizer确实很重要。没有这俩东西在14个epoch就early-stop了
Correction Precision: 0.6147368421051552, Recall: 0.650092764378358, F1-Score: 0.6319206487338203
Correction Precision: 0.6328730212191591, Recall: 0.6963127663515478, F1-Score: 0.663078958477643
Correction Precision: 0.6428099173552656, Recall: 0.719518963922161, F1-Score: 0.6790048008982366
Correction Precision: 0.6482501620219299, Recall: 0.7405145289652525, F1-Score: 0.6913174941025193
Correction Precision: 0.6527597402596342, Recall: 0.7447675495460743, F1-Score: 0.6957349246685837
Correction Precision: 0.6530777976286092, Recall: 0.7450435427087743, F1-Score: 0.6960360043488492
Correction Precision: 0.6545277190699634, Recall: 0.7455555555554174, F1-Score: 0.6970825031813279
Correction Precision: 0.6554379210778544, Recall: 0.7570872707058074, F1-Score: 0.7026051065439953
Correction Precision: 0.657434402332255, Recall: 0.7518058899794865, F1-Score: 0.7014602950175447
Correction Precision: 0.6601159793813369, Recall: 0.7586079229913442, F1-Score: 0.7059431519570685
Correction Precision: 0.658473479948147, Recall: 0.7537948907810526, F1-Score: 0.7029173135018112
Correction Precision: 0.6585562359006351, Recall: 0.7572725588288387, F1-Score: 0.7044729806278802
Correction Precision: 0.660209846650418, Recall: 0.757688032604528, F1-Score: 0.7055982053160944
Correction Precision: 0.6597738287559515, Recall: 0.7564363771067315, F1-Score: 0.704806281321174
Correction Precision: 0.6599001127757919, Recall: 0.758518518518378, F1-Score: 0.7057809937299167

模型：MacBert4Csc，使用了源码中的optimizer（AdamW），但还未使用schedule
执行情况：和不用这个optimizer效果差不多，看来optimizer还是要和schedule配合起来才行，属于联合超参数。
Correction Precision: 0.6342253281062494, Recall: 0.6903525046380908, F1-Score: 0.6610997596500208
Correction Precision: 0.6486311355903696, Recall: 0.7418936446172425, F1-Score: 0.6921348309628036
Correction Precision: 0.6541268039564367, Recall: 0.746345975948058, F1-Score: 0.6972001377668244
Correction Precision: 0.6555466879488179, Recall: 0.7601332593002479, F1-Score: 0.7039766878810791
Correction Precision: 0.6603803739810356, Recall: 0.7653269123910418, F1-Score: 0.7089910770592107
Correction Precision: 0.6593090211131383, Recall: 0.7637576431349334, F1-Score: 0.7077002312821239
Correction Precision: 0.6595471334509941, Recall: 0.7605555555554147, F1-Score: 0.7064591033125052
Correction Precision: 0.6608318612492916, Recall: 0.7624606262737145, F1-Score: 0.7080178935148156
Correction Precision: 0.6632049848217505, Recall: 0.7688460826077479, F1-Score: 0.7121290096244034
Correction Precision: 0.6634553628772216, Recall: 0.7649018881894177, F1-Score: 0.7105760958050654
Correction Precision: 0.6602717825738352, Recall: 0.7645316549424722, F1-Score: 0.7085871145407301
Correction Precision: 0.663052144793388, Recall: 0.7704280155640595, F1-Score: 0.7127185459545171
Correction Precision: 0.6616129032256997, Recall: 0.7610389610388197, F1-Score: 0.7078515957059437
Correction Precision: 0.6592948717947661, Recall: 0.7622753381506833, F1-Score: 0.7070550824276547
Correction Precision: 0.6607228915661588, Recall: 0.7609620721552708, F1-Score: 0.7073086839391652
Correction Precision: 0.6605356566633332, Recall: 0.7577271885987862, F1-Score: 0.7058012235346374
Epoch 16 Training: 100% 250/250 [02:35<00:00,  1.60it/s, loss=0.0109, c_precision=0.732, c_recall=0.997, c_f1_score=0.844]
Correction Precision: 0.6601675797614147, Recall: 0.7588442304128988, F1-Score: 0.706074967188638

模型：MacBert4Csc，完全模拟了MacBert4Csc的训练流程
执行情况：看起来效果并不好，有两种可能性。一是在大数据量上表现会好，二是在我的代码有问题
Correction Precision: 0.5704747774479654, Recall: 0.5706864564006362, F1-Score: 0.5705805967916766
Correction Precision: 0.6170952050033673, Recall: 0.6581434130071042, F1-Score: 0.6369586653302253
Correction Precision: 0.6379776800810554, Recall: 0.6980573543014434, F1-Score: 0.6666666661675599
Correction Precision: 0.6459011060506431, Recall: 0.7349620581157255, F1-Score: 0.6875595181583541
Correction Precision: 0.6516744863289675, Recall: 0.7460640859417028, F1-Score: 0.6956822102102774
Correction Precision: 0.6543369407202948, Recall: 0.7506021863997127, F1-Score: 0.6991715562851054
Correction Precision: 0.6557825317676281, Recall: 0.7549999999998601, F1-Score: 0.7019023839389535
Correction Precision: 0.6571337274323922, Recall: 0.762090050027652, F1-Score: 0.705730953517713
Correction Precision: 0.6591712174749985, Recall: 0.7601407668085274, F1-Score: 0.7060645156314411
Correction Precision: 0.6602564102563044, Recall: 0.7626804887077447, F1-Score: 0.7077821675148379
Correction Precision: 0.6600064453753367, Recall: 0.7582376897443986, F1-Score: 0.7057201924726422
Correction Precision: 0.6613603473226143, Recall: 0.762090050027652, F1-Score: 0.708161156527175
Correction Precision: 0.6618635926992216, Recall: 0.7658391997034519, F1-Score: 0.7100652691693109
Correction Precision: 0.66088353413644, Recall: 0.7619929616594254, F1-Score: 0.7078458357033665
Correction Precision: 0.6600543912972868, Recall: 0.7640740740739325, F1-Score: 0.7082653844480441
Correction Precision: 0.6623272259722497, Recall: 0.763006850583084, F1-Score: 0.7091112444477395
Epoch 16 Training: 100% 250/250 [02:33<00:00,  1.63it/s, loss=0.00943, c_precision=0.728, c_recall=0.997, c_f1_score=0.841]
Correction Precision: 0.6621248986211415, Recall: 0.7550869404364122, F1-Score: 0.7055569954401505

Evel情况：经过一次全量数据的训练后，在sighan上的character-level如下，效果还是想当好的。看来optimizer和scheduler非常重要啊。
The detection result is precision=0.7780979827089337, recall=0.7670454545454546 and F1=0.7725321888412018
The correction result is precision=0.924074074074074, recall=0.8414839797639123 and F1=0.880847308031774
Sentence Level: acc:0.7482, precision:0.8103, recall:0.6384, f1:0.7141, total num: 1100


模型：MultiModalMacBert4Csc，完全模拟了MacBert4Csc的训练流程，但Bert增加了pinyin和glyph embeddings。因为这样，macbert的预测层初始化没法使用原有的参数。
执行情况：比原本的MacBert4Csc要好。
Correction Precision: 0.002427376586369689, Recall: 0.01708449396471363, F1-Score: 0.004250796806584928
Correction Precision: 0.13678495762710052, Recall: 0.19200743494420222, F1-Score: 0.15975873752830805
Correction Precision: 0.39685501066087503, Recall: 0.276508820798463, F1-Score: 0.32592754685091124
Correction Precision: 0.6617730095989097, Recall: 0.4350408314772763, F1-Score: 0.5249720040005319
Correction Precision: 0.7793904208996663, Recall: 0.5985509938694782, F1-Score: 0.677104128961028
Correction Precision: 0.84168865435336, Recall: 0.6527157738094024, F1-Score: 0.7352540592249704
Correction Precision: 0.8501710376280842, Recall: 0.6930656255808341, F1-Score: 0.7636214661170696
Correction Precision: 0.8714416896233077, Recall: 0.7054450845566427, F1-Score: 0.7797062745387583
Correction Precision: 0.8797297297295315, Recall: 0.7250788936326851, F1-Score: 0.7949526808924939
Correction Precision: 0.8846153846151878, Recall: 0.7375347544020875, F1-Score: 0.8044071560793041
Correction Precision: 0.8910514541385031, Recall: 0.7407476287891499, F1-Score: 0.8089773530127605
Correction Precision: 0.8983430362738695, Recall: 0.7461409708014234, F1-Score: 0.8151986178115439
Correction Precision: 0.8966202783298217, Recall: 0.7546012269937247, F1-Score: 0.8195033308178881
Correction Precision: 0.8888166449933054, Recall: 0.7621259988848239, F1-Score: 0.8206103046553564
Correction Precision: 0.89431248638028, Recall: 0.7625418060199252, F1-Score: 0.8231872425077109
Epoch 16 Training:   1% 3/250 [00:02<03:23,  1.21it/s, loss=0.00413, c_precision=1, c_recall=0.976, c_f1_score=0.988]
Correction Precision: 0.8970331588130678, Recall: 0.7631774313287373, F1-Score: 0.8247091852231009

一次全量数据Eval情况：一次全量eval后，表现不并如原本的MacBert4Csc好，我觉得可能是pinyin和glphy信息确实有效，而且太有效了，导致过拟合了Wang271K数据，
所以在Sighan数据集上表现不好，但在Wang271K的测试集上表现很好。
Character-level Detect Acc: 0.9871, P: 0.7178, R: 0.6250, F1: 0.6682
Character-level Correct Acc: 0.9856, P: 0.6919, R: 0.5517, F1: 0.6139
Sentence-level Detect Acc: 0.7044, P: 0.7761, R: 0.5638, F1: 0.6531
Sentence-level Correct Acc: 0.6734, P: 0.7549, R: 0.5009, F1: 0.6022


模型：MultiModalMacBert4Csc，完全模拟了MacBert4Csc的训练流程，但Bert增加了pinyin和glyph embeddings。
因为这样，macbert的预测层初始化没法使用原有的参数。Loss使用了0.25的loss和0.75的soft loss
执行情况：

