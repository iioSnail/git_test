实验：1w Wang271K数据，8:2拆分，20epoch。看验证集Correction情况

baseline: 纯BERT（chinese-roberta-wwm-ext）
执行情况：花费1小时
Correction Precision: 0.05530088044822416, Recall: 0.1420560747663286, F1-Score: 0.07961032798921384
Correction Precision: 0.17577108881454528, Recall: 0.26503548748594224, F1-Score: 0.21136515927320912
Correction Precision: 0.24929044465464373, Recall: 0.29556926528317523, F1-Score: 0.2704644593444641
Correction Precision: 0.2969277215414141, Recall: 0.3124065769805096, F1-Score: 0.30447054488859465
Correction Precision: 0.3403013182673558, Recall: 0.33718977421154367, F1-Score: 0.3387384004747338
Correction Precision: 0.3632075471697399, Recall: 0.34516249533052945, F1-Score: 0.3539551805000651
Correction Precision: 0.4058879392211954, Recall: 0.39869402985067187, F1-Score: 0.402258823029376
Correction Precision: 0.45856052344593623, Recall: 0.4722066254912086, F1-Score: 0.4652835403022346
Correction Precision: 0.49203570803422153, Recall: 0.5248319641522544, F1-Score: 0.5079049592980612
Correction Precision: 0.5215438363000297, Recall: 0.5862004487657841, F1-Score: 0.5519852094672721
Correction Precision: 0.5429462966003378, Recall: 0.618406285072836, F1-Score: 0.578224748080934
Correction Precision: 0.5549999999999105, Recall: 0.6428171118997491, F1-Score: 0.5956894308191255
Correction Precision: 0.5656014132004873, Recall: 0.6583177570092227, F1-Score: 0.6084477839028679
Correction Precision: 0.5699383594119535, Recall: 0.6741446999437887, F1-Score: 0.617677286245437
Correction Precision: 0.5773519711008988, Recall: 0.6862049654656176, F1-Score: 0.627089729971021
Correction Precision: 0.5831769856159188, Recall: 0.6964152352500567, F1-Score: 0.6347855679176947
Correction Precision: 0.5861315008588808, Recall: 0.7012331838563711, F1-Score: 0.6385367923579929
Correction Precision: 0.5908596300325523, Recall: 0.7100691201194265, F1-Score: 0.6450025448969279
Correction Precision: 0.5956122607747634, Recall: 0.7147124719938919, F1-Score: 0.6497496388148921
Correction Precision: 0.6004063134863884, Recall: 0.71853375724692, F1-Score: 0.6541801459367275


模型：MDCSpell
执行情况：效果确实比只用bert好很多，到15个epoch时，基本已经可以完全拟合训练数据了。
Correction Precision: 0.9830810329470244, Recall: 0.41495959406118865, F1-Score: 0.5835866257222314
Correction Precision: 0.9846205282510917, Recall: 0.5529478032293366, F1-Score: 0.7081880481144472
Correction Precision: 0.9842073897494088, Recall: 0.619932432432316, F1-Score: 0.7607093500553181
Correction Precision: 0.9785360484312112, Recall: 0.6676680435597694, F1-Score: 0.7937499995176529
Correction Precision: 0.9826203208553522, Recall: 0.6896228185399343, F1-Score: 0.8104531917083564
Correction Precision: 0.9818229031417861, Recall: 0.7100469483566741, F1-Score: 0.8241063639416675
Correction Precision: 0.9806240563661347, Recall: 0.7308702175542514, F1-Score: 0.8375241774601772
Correction Precision: 0.9815645241651765, Recall: 0.7396283086163432, F1-Score: 0.8435927625971622
Correction Precision: 0.9824518042508694, Recall: 0.7463387157339942, F1-Score: 0.8482714463721418
Correction Precision: 0.9801373222165326, Recall: 0.7506103286383566, F1-Score: 0.8501542056130476
Epoch 10 Training: 100% 250/250 [01:41<00:00,  2.47it/s, loss=0.0211, c_precision=0.999, c_recall=0.979, c_f1_score=0.989]
Correction Precision: 0.9824347401802921, Recall: 0.7568126292048943, F1-Score: 0.8549893837969799
Epoch 11 Training: 100% 250/250 [02:41<00:00,  1.55it/s, loss=0.014, c_precision=0.999, c_recall=0.983, c_f1_score=0.991]
Correction Precision: 0.9841540711845479, Recall: 0.7579797221177698, F1-Score: 0.856385234977062
Epoch 12 Training: 100% 250/250 [02:42<00:00,  1.54it/s, loss=0.0131, c_precision=0.999, c_recall=0.991, c_f1_score=0.995]
Correction Precision: 0.9851653696495658, Recall: 0.76032282282268, F1-Score: 0.8582627113725213
Epoch 13 Training: 100% 250/250 [02:44<00:00,  1.52it/s, loss=0.0154, c_precision=0.999, c_recall=0.995, c_f1_score=0.997]
Correction Precision: 0.9823330106483584, Recall: 0.7621104018023352, F1-Score: 0.8583209976046362
Epoch 14 Training: 100% 250/250 [02:44<00:00,  1.52it/s, loss=0.0102, c_precision=0.999, c_recall=0.997, c_f1_score=0.998]
Correction Precision: 0.9842843326883499, Recall: 0.7639331957213803, F1-Score: 0.8602218695553052
Epoch 15 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.0114, c_precision=0.999, c_recall=0.997, c_f1_score=0.998]
Correction Precision: 0.983075435202857, Recall: 0.7635680751172275, F1-Score: 0.8595285905657428
Epoch 16 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.00963, c_precision=0.999, c_recall=0.998, c_f1_score=0.999]
Correction Precision: 0.9833574529664776, Recall: 0.7646286571641476, F1-Score: 0.8603080813814604
Epoch 17 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.01, c_precision=1, c_recall=0.999, c_f1_score=0.999]
Correction Precision: 0.9850313858037215, Recall: 0.7659095175519492, F1-Score: 0.8617594250013648
Epoch 18 Training: 100% 250/250 [02:46<00:00,  1.50it/s, loss=0.00942, c_precision=1, c_recall=0.999, c_f1_score=1]
Correction Precision: 0.9836499158449185, Recall: 0.7681186631617033, F1-Score: 0.8626251971879352
Epoch 19 Training: 100% 250/250 [02:45<00:00,  1.51it/s, loss=0.0081, c_precision=1, c_recall=0.999, c_f1_score=0.999]
Correction Precision: 0.9847972972970596, Recall: 0.7663849765256776, F1-Score: 0.8619706405467762
Eval：
Detection Character-level Precision 0.8703703703671468, Recall 0.33428165007064825, F1_Score 0.48304213731642315
Correction Character-level Precision 0.8648648648615257, Recall 0.32369942196485013, F1_Score 0.47108307005481853
Detection Sentence-level Precision 0.7654320987339328, Recall 0.343807763394754, F1_Score 0.4744897916286509
Correction Sentence-level Precision 0.7283950616984199, Recall 0.32717190387565304, F1_Score 0.4515306079557672

模型：MDCSpellPlus，MDCSpell使用MultiModalBert作为Bert
执行情况：看起来是比单纯的使用Bert效果好，大概在第28个epoch完全拟合了训练集，最终效果没有原本的MDCSpell效果好
Correction Precision: 0.10402249134946971, Recall: 0.17998129092606546, F1-Score: 0.13184403434695938
Correction Precision: 0.33396494552336947, Recall: 0.2643419572552935, F1-Score: 0.2951025528763196
Correction Precision: 0.4976198032369097, Recall: 0.2933034044144606, F1-Score: 0.36907143651103624
Correction Precision: 0.5975895547369945, Recall: 0.3342696629212857, F1-Score: 0.42872583117186164
Correction Precision: 0.7018147684603562, Recall: 0.420273561926097, F1-Score: 0.5257236606118652
Correction Precision: 0.7784496976358498, Recall: 0.5310331895742488, F1-Score: 0.6313677400148663
Correction Precision: 0.8336870026522988, Recall: 0.5887973023603242, F1-Score: 0.6901624940249879
Correction Precision: 0.8389604080639205, Recall: 0.6465743167351841, F1-Score: 0.7303097574107581
Correction Precision: 0.8692345436700517, Recall: 0.663980509745003, F1-Score: 0.7528686777918437
Correction Precision: 0.8752422480618034, Recall: 0.6769720816937086, F1-Score: 0.7634442678651017
Correction Precision: 0.8778013682470209, Recall: 0.6968164794006185, F1-Score: 0.7769078186942497
Correction Precision: 0.8799628511723054, Recall: 0.7100037467214855, F1-Score: 0.7858994292619005
Correction Precision: 0.8836889194767414, Recall: 0.7210782478471132, F1-Score: 0.7941449330169791
Correction Precision: 0.880952380952183, Recall: 0.7345944933506771, F1-Score: 0.8011439071742492
Correction Precision: 0.8962808962806892, Recall: 0.7265917602994894, F1-Score: 0.8025648976333385
Correction Precision: 0.8918918918916893, Recall: 0.7364966241559008, F1-Score: 0.8067796605213365
Correction Precision: 0.8986995208759071, Recall: 0.7370883233531554, F1-Score: 0.8099105577446644
Correction Precision: 0.8981941309253051, Recall: 0.7455499344199464, F1-Score: 0.8147844778495938
Correction Precision: 0.9041002277902268, Recall: 0.742841100505195, F1-Score: 0.8155758753907864
Correction Precision: 0.9057414104880411, Recall: 0.7502340385694157, F1-Score: 0.8206861234161715
Correction Precision: 0.906618313689731, Recall: 0.7483629560335363, F1-Score: 0.8199241565198798
Correction Precision: 0.9018336314845925, Recall: 0.7560929883763111, F1-Score: 0.8225576172887252
Correction Precision: 0.9109311740888638, Recall: 0.7575757575756158, F1-Score: 0.8272058818569961
Correction Precision: 0.9015405224378429, Recall: 0.7561797752807572, F1-Score: 0.8224870144746514
Correction Precision: 0.9037647582978605, Recall: 0.7601648866402922, F1-Score: 0.8257683691351455
Correction Precision: 0.9168949771687404, Recall: 0.7530470654414488, F1-Score: 0.8269329759279931
Correction Precision: 0.9074280615657132, Recall: 0.7620831772197897, F1-Score: 0.8284288764000671
Correction Precision: 0.9117381489839927, Recall: 0.7560838637213111, F1-Score: 0.8266475639740999
Eval:
Detection Character-level Precision 0.44703143189703487, Recall 0.5462304409665061, F1_Score 0.49167733625211696
Correction Character-level Precision 0.39949431099823074, Recall 0.4976377952748069, F1_Score 0.4431977554660916
Detection Sentence-level Precision 0.34090909090355664, Recall 0.38817005544569, F1_Score 0.3630077737528508
Correction Sentence-level Precision 0.282467532462947, Recall 0.321626617369286, F1_Score 0.3007778688273925

