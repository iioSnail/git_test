# FIXME d_loss忘了对pad进行mask了
模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。Detection模块的head也是macbert的head，detection模块在head前一层把特征加到correction模块中去。d_loss和loss分别是0.3,0.7。
代码：MultiModalBertWithDetection1.py
执行情况：第14个epoch我手动停了，看起来不会再增长了，虽然还没有过拟合训练集。效果比之前要好1个点，但并没有好太多。
Correction Precision: 0.4484823456534279, Recall: 0.40712277413300707, F1-Score: 0.42680290773452917
Correction Precision: 0.8068241469814156, Recall: 0.5764110256889975, F1-Score: 0.6724269928419558
Correction Precision: 0.8784835551089936, Recall: 0.6570892018778108, F1-Score: 0.75182638541483
Correction Precision: 0.9105275954289057, Recall: 0.7010482965180267, F1-Score: 0.7921734527076457
Correction Precision: 0.9254163649527092, Recall: 0.7194595608930907, F1-Score: 0.8095439184265869
Correction Precision: 0.9335225928552321, Recall: 0.7403377110692794, F1-Score: 0.8257821487164902
Correction Precision: 0.9367058823527207, Recall: 0.7457849381789535, F1-Score: 0.8304130157766029
Correction Precision: 0.9383208255157273, Recall: 0.7507975229872863, F1-Score: 0.8341499004754453
Correction Precision: 0.9380717804360923, Recall: 0.7494377811093048, F1-Score: 0.8332117924011672
Correction Precision: 0.9387277829745231, Recall: 0.7525309336331547, F1-Score: 0.8353798122009956
Correction Precision: 0.938713450292178, Recall: 0.7519205546185587, F1-Score: 0.8349979187735119
Correction Precision: 0.9391243268553173, Recall: 0.7518275538892686, F1-Score: 0.8351030600930934
Correction Precision: 0.9382311651846189, Recall: 0.7513584410716223, F1-Score: 0.834460513500324
Epoch 13 Training: 100% 250/250 [03:42<00:00,  1.12it/s, loss=0.00926, c_precision=0.991, c_recall=0.882, c_f1_score=0.933]
Correction Precision: 0.9379971923254706, Recall: 0.7517344834050718, F1-Score: 0.834599770506214
Eval情况：eval情况也有提升，大概3个点，但是依然不明显。
Character-level Detect Acc: 0.9836, P: 0.6266, R: 0.5460, F1: 0.5835
Character-level Correct Acc: 0.9813, P: 0.5749, R: 0.4399, F1: 0.4984
Sentence-level Detect Acc: 0.6445, P: 0.7115, R: 0.4752, F1: 0.5699
Sentence-level Correct Acc: 0.6018, P: 0.6688, R: 0.3890, F1: 0.4919

# FIXME d_loss忘了对pad进行mask了
模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。Detection模块的head也是macbert的head，detection模块在head前一层把特征加到correction模块中去。d_loss和loss分别是0.3,0.7。
调整为：0.15的loss和0.75的soft_loss
执行情况：第9个epoch我手动停了，看起来不会再增长了，虽然还没有过拟合训练集。效果不如0.99soft_loss好
Correction Precision: 0.4555620953501166, Recall: 0.4352389878162258, F1-Score: 0.44516871115661655
Correction Precision: 0.8037525354967536, Recall: 0.594412150759311, F1-Score: 0.683410584829591
Correction Precision: 0.8753970193010322, Recall: 0.6728638497651318, F1-Score: 0.7608834142461307
Correction Precision: 0.9041816009555783, Recall: 0.7083489329837686, F1-Score: 0.7943738842558166
Correction Precision: 0.9223462360482254, Recall: 0.7288421842745864, F1-Score: 0.8142557647058574
Correction Precision: 0.9259522119704456, Recall: 0.7343339587240648, F1-Score: 0.8190854865840234
Correction Precision: 0.9290261730721694, Recall: 0.7381041588608583, F1-Score: 0.8226328421830743
Correction Precision: 0.9301940369141196, Recall: 0.7376618502531923, F1-Score: 0.8228152794646276
Epoch 8 Training: 100% 250/250 [03:36<00:00,  1.15it/s, loss=0.00624, c_precision=0.986, c_recall=0.847, c_f1_score=0.911]
Correction Precision: 0.9292118924018571, Recall: 0.7380059970013609, F1-Score: 0.8226446621343552
Eval情况：eval情况也变差了，看来单独计算纠错loss要慎重
Character-level Detect Acc: 0.9816, P: 0.5680, R: 0.5318, F1: 0.5493
Character-level Correct Acc: 0.9793, P: 0.5103, R: 0.4215, F1: 0.4617
Sentence-level Detect Acc: 0.6109, P: 0.6585, R: 0.4459, F1: 0.5317
Sentence-level Correct Acc: 0.5673, P: 0.6075, R: 0.3578, F1: 0.4503
Detection Eval情况：看起来Detection模块是有被训练的
Detection Character-level Precision 0.5966101694905143, Recall 0.49787835926379365, F1_Score 0.5427910557869634

# FIXME d_loss忘了对pad进行mask了
模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。Detection模块的head也是macbert的head。d_loss和loss分别是0.3,0.7。
相比相加融合，本次使用了concat进行融合（因为concat了，correction的head的网络会变宽）。
执行情况：执行了10个epoch后手动停了，很难再涨了。效果相比相加融合差了0.5个点
Correction Precision: 0.7256157635466193, Recall: 0.5529279279278241, F1-Score: 0.6276097140381738
Correction Precision: 0.8818204676889912, Recall: 0.6582207207205971, F1-Score: 0.7537882853781911
Correction Precision: 0.9210718635807743, Recall: 0.7087160262416665, F1-Score: 0.8010593215422169
Correction Precision: 0.9278375149340673, Recall: 0.7281080067502853, F1-Score: 0.8159277154138053
Correction Precision: 0.9314649081486265, Recall: 0.7423048048046654, F1-Score: 0.8261959468636398
Correction Precision: 0.9310183012667923, Recall: 0.7443256424684404, F1-Score: 0.8272698837966649
Correction Precision: 0.9331456720617092, Recall: 0.7455022488754225, F1-Score: 0.8288363366245247
Correction Precision: 0.9333958724200436, Recall: 0.7458770614691255, F1-Score: 0.8291666661727284
Correction Precision: 0.9344454887215848, Recall: 0.7458739684919831, F1-Score: 0.8295786394727489
Epoch 9 Training: 100% 250/250 [03:56<00:00,  1.06it/s, loss=0.00626, c_precision=0.995, c_recall=0.924, c_f1_score=0.958]
Correction Precision: 0.9338494018294783, Recall: 0.7462043111526249, F1-Score: 0.8295478219690776
Eval情况：Eval情况也不如相加融合
Character-level Detect Acc: 0.9835, P: 0.6226, R: 0.5530, F1: 0.5858
Character-level Correct Acc: 0.9808, P: 0.5587, R: 0.4243, F1: 0.4823
Sentence-level Detect Acc: 0.6336, P: 0.6983, R: 0.4587, F1: 0.5537
Sentence-level Correct Acc: 0.5836, P: 0.6436, R: 0.3578, F1: 0.4599
Detection Eval情况：看起来Detection模块是有被训练的，但效果不够好。
Detection Character-level Precision 0.5982608695641769, Recall 0.48656294200779837, F1_Score 0.5366614659631219


模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。Detection模块的head也是macbert的head，detection模块在head前一层把特征加到correction模块中去。d_loss和loss分别是0.3,0.7。
将Detection的head的激活函数从GELU改为了Tanh。
执行情况：13个epoch后停了。效果不如GELU好。
Correction Precision: 0.45574582215794185, Recall: 0.4140581068415343, F1-Score: 0.43390296551952723
Correction Precision: 0.8093487394955858, Recall: 0.5779111194448568, F1-Score: 0.6743244716721759
Correction Precision: 0.8814328960643589, Recall: 0.6561502347416608, F1-Score: 0.7522876515721499
Correction Precision: 0.9117073170729483, Recall: 0.6997379258703295, F1-Score: 0.7917814017537705
Correction Precision: 0.9258724428397289, Recall: 0.7218990429722796, F1-Score: 0.8112610707854322
Correction Precision: 0.930472308798626, Recall: 0.7281425891180622, F1-Score: 0.8169666345983114
Correction Precision: 0.9325681492106808, Recall: 0.7306107156236922, F1-Score: 0.8193277305996371
Correction Precision: 0.9332376166544787, Recall: 0.731844623756665, F1-Score: 0.8203618000961195
Correction Precision: 0.9330143540667624, Recall: 0.7308845577210024, F1-Score: 0.8196721306547473
Correction Precision: 0.9336357125803452, Recall: 0.7332208473939382, F1-Score: 0.8213798167915247
Correction Precision: 0.9333333333331102, Recall: 0.7318718381111613, F1-Score: 0.8204158785241592
Correction Precision: 0.9340028694402357, Recall: 0.7321462043110155, F1-Score: 0.8208469050446255
Epoch 12 Training: 100% 250/250 [03:33<00:00,  1.17it/s, loss=0.0215, c_precision=0.986, c_recall=0.82, c_f1_score=0.895]
Correction Precision: 0.93307839388123, Recall: 0.731497095746537, F1-Score: 0.8200819236747898
Eval情况：不太行
Character-level Detect Acc: 0.9822, P: 0.5864, R: 0.5233, F1: 0.5531
Character-level Correct Acc: 0.9798, P: 0.5272, R: 0.4116, F1: 0.4623
Sentence-level Detect Acc: 0.6218, P: 0.6817, R: 0.4440, F1: 0.5378
Sentence-level Correct Acc: 0.5764, P: 0.6295, R: 0.3523, F1: 0.4518
Detection Eval情况：
Detection Character-level Precision 0.5947281713334519, Recall 0.5106082036767884, F1_Score 0.5494672749967322

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。d_loss和loss分别是0.3,0.7。
Detection模块和Correction模块前（BERT后）加了一个共同的Head，然后Detection和Correction各学各的，目的是：让head具有改错和纠错的共同特征。
代码：MultiModalBertWithDetection2.py
执行情况：10个epoch后停了。Wang271K上并没有好太多。
Correction Precision: 0.4254560704549328, Recall: 0.3784035807533796, F1-Score: 0.40055275836058174
Correction Precision: 0.7923017308187051, Recall: 0.5715616846812203, F1-Score: 0.6640684199957945
Correction Precision: 0.8816223067171403, Recall: 0.6482758620688446, F1-Score: 0.747153597792889
Correction Precision: 0.9038508113342445, Recall: 0.6948426736174464, F1-Score: 0.7856842100346964
Correction Precision: 0.9245689655170198, Recall: 0.7193963107880157, F1-Score: 0.8091795028084671
Correction Precision: 0.9300782175868854, Recall: 0.7319529938442955, F1-Score: 0.8192066800914851
Correction Precision: 0.9329425556855959, Recall: 0.7405546249765976, F1-Score: 0.8256899766801242
Correction Precision: 0.9343611305767496, Recall: 0.7457121551079892, F1-Score: 0.8294453079561062
Correction Precision: 0.9344569288387324, Recall: 0.7442207307977732, F1-Score: 0.8285595677918448
Epoch 9 Training: 100% 250/250 [03:29<00:00,  1.19it/s, loss=0.00584, c_precision=0.991, c_recall=0.906, c_f1_score=0.947]
Correction Precision: 0.934096751577253, Recall: 0.7445976154991161, F1-Score: 0.828651393722018
Eval情况：这个比MultiModalBertWithDetection1.py好了一丢丢（可以忽略不计）
Character-level Detect Acc: 0.9842, P: 0.6514, R: 0.5417, F1: 0.5915
Character-level Correct Acc: 0.9822, P: 0.6050, R: 0.4441, F1: 0.5122
Sentence-level Detect Acc: 0.6500, P: 0.7260, R: 0.4716, F1: 0.5717
Sentence-level Correct Acc: 0.6064, P: 0.6830, R: 0.3835, F1: 0.4912
Detection Eval情况：也比之前好一点，但不多
Detection Character-level Precision 0.5806916426504601, Recall 0.5700141442707638, F1_Score 0.5753033542458312

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。d_loss和loss分别是0.3,0.7。Detection模块和Correction模块前（BERT后）加了一个共同的Head，然后Detection和Correction各学各的，目的是：让head具有改错和纠错的共同特征。
增加了残差链接，bert后的hidden_state加到了head的输出上。
执行情况：训练了15个epoch后停了。比不加残差连接好一点
Correction Precision: 0.4171299155856666, Recall: 0.37784408802678515, F1-Score: 0.3965162926805152
Correction Precision: 0.7893655278703341, Recall: 0.5726798360043658, F1-Score: 0.6637865855369198
Correction Precision: 0.8806731259559203, Recall: 0.6438024231126479, F1-Score: 0.7438354684469815
Correction Precision: 0.9032727272725082, Recall: 0.6937255632097014, F1-Score: 0.7847514738134136
Correction Precision: 0.9246411483251376, Recall: 0.7201416061112874, F1-Score: 0.8096784325231665
Correction Precision: 0.9296838131191765, Recall: 0.7349375116581356, F1-Score: 0.8209188452197674
Correction Precision: 0.9337856808608015, Recall: 0.7427880141446598, F1-Score: 0.8274074836982942
Correction Precision: 0.9353257062804259, Recall: 0.7468307233406511, F1-Score: 0.8305172587576867
Correction Precision: 0.934433852592203, Recall: 0.7492542878447521, F1-Score: 0.8316606306491767
Correction Precision: 0.9353939112245094, Recall: 0.749813710879145, F1-Score: 0.8323854818758633
Correction Precision: 0.9365706319700425, Recall: 0.7516315495057334, F1-Score: 0.8339712418767804
Correction Precision: 0.9360613810739511, Recall: 0.7505592841161911, F1-Score: 0.8331091562570399
Correction Precision: 0.9356263072272981, Recall: 0.7501397428729737, F1-Score: 0.8326783862690661
Correction Precision: 0.936363636363418, Recall: 0.7498599962664271, F1-Score: 0.8327977604679011
Epoch 14 Training: 100% 250/250 [03:32<00:00,  1.18it/s, loss=0.00569, c_precision=0.993, c_recall=0.902, c_f1_score=0.945]
Correction Precision: 0.9354688950787057, Recall: 0.7499069594341737, F1-Score: 0.8324726291278679
Eval情况：首次突破50大关，比不加残差链接好了2个点。
Character-level Detect Acc: 0.9848, P: 0.6701, R: 0.5488, F1: 0.6034
Character-level Correct Acc: 0.9828, P: 0.6262, R: 0.4526, F1: 0.5255
Sentence-level Detect Acc: 0.6636, P: 0.7410, R: 0.4936, F1: 0.5925
Sentence-level Correct Acc: 0.6200, P: 0.7016, R: 0.4055, F1: 0.5140
Detection Eval情况：Detection情况也有所好转，但不多。
Detection Character-level Precision 0.6168224299055813, Recall 0.5601131541717679, F1_Score 0.5871015562089635


模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。d_loss和loss分别是0.3,0.7。Detection模块和Correction模块前（BERT后）加了一个共同的Head，然后Detection和Correction各学各的，目的是：让head具有改错和纠错的共同特征。增加了残差链接，bert后的hidden_state加到了head的输出上。
Correct模型融合了Detection的特征。Detection head的输出加到了Head的输出上后，送给Correction的预测层。
执行情况：执行了13个epoch后手动停了，感觉有没有这个效果没啥差别，都是0.832左右
Correction Precision: 0.4183572976166203, Recall: 0.3961208504288705, F1-Score: 0.40693552972349617
Correction Precision: 0.78684807256216, Recall: 0.5819977636972452, F1-Score: 0.669094804010251
Correction Precision: 0.8730836893689689, Recall: 0.6475302889094785, F1-Score: 0.7435787666341294
Correction Precision: 0.8905318387787048, Recall: 0.6952150437533615, F1-Score: 0.780844834304578
Correction Precision: 0.9216203259825211, Recall: 0.7164151294949289, F1-Score: 0.8061641676594757
Correction Precision: 0.9274952919018531, Recall: 0.7349375116581356, F1-Score: 0.8200645223497053
Correction Precision: 0.9304911955512208, Recall: 0.7474409082447891, F1-Score: 0.8289813185273502
Correction Precision: 0.931090487238763, Recall: 0.7481357196120901, F1-Score: 0.829646474593628
Correction Precision: 0.9318549318547165, Recall: 0.7520507084264071, F1-Score: 0.832353244115041
Correction Precision: 0.9325011558019111, Recall: 0.7523312196940036, F1-Score: 0.8327828236178495
Correction Precision: 0.9325790810434235, Recall: 0.7527021990307952, F1-Score: 0.8330411462519406
Epoch 12 Training: 100% 250/250 [03:26<00:00,  1.21it/s, loss=0.00722, c_precision=0.991, c_recall=0.925, c_f1_score=0.957]
Correction Precision: 0.9321016166279602, Recall: 0.7522833178004189, F1-Score: 0.8325941201862898
Eval情况：确实没啥差别。
Character-level Detect Acc: 0.9845, P: 0.6545, R: 0.5601, F1: 0.6037
Character-level Correct Acc: 0.9823, P: 0.6064, R: 0.4554, F1: 0.5202
Sentence-level Detect Acc: 0.6655, P: 0.7373, R: 0.5046, F1: 0.5991
Sentence-level Correct Acc: 0.6173, P: 0.6937, R: 0.4073, F1: 0.5133
Detection Eval情况：甚至差了1个点。这也可以理解，因为被干扰了。
Detection Character-level Precision 0.5922038980500867, Recall 0.5586987270147684, F1_Score 0.5749636093976944


模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。d_loss和loss分别是0.3,0.7。Detection模块和Correction模块前（BERT后）加了一个共同的Head，然后Detection和Correction各学各的，目的是：让head具有改错和纠错的共同特征。增加了残差链接，bert后的hidden_state加到了head的输出上。
对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。
执行情况：没啥效果，但也没咋降。原因可能是：用多任务和index1的效果重叠了，它们都是让模型具有纠错的特征，所以两者结合用和单独用其中一者效果都一样。
Correction Precision: 0.9171974522263784, Recall: 0.05371130175306719, F1-Score: 0.1014799153288475
Correction Precision: 0.9635395329778929, Recall: 0.43831531867304535, F1-Score: 0.6025361850446062
Correction Precision: 0.9654305468255923, Recall: 0.5726001863931831, F1-Score: 0.718848718381168
Correction Precision: 0.964566929133587, Recall: 0.6386147830942769, F1-Score: 0.7684552476441661
Correction Precision: 0.9699067909451479, Recall: 0.6785913918388897, F1-Score: 0.7985090983972631
Correction Precision: 0.9563783920870007, Recall: 0.7034135422494491, F1-Score: 0.8106190880754982
Correction Precision: 0.9632663191856631, Recall: 0.7223152801040903, F1-Score: 0.8255690273764528
Correction Precision: 0.9614906832295748, Recall: 0.7214765100669795, F1-Score: 0.8243689418893737
Correction Precision: 0.9639482844353645, Recall: 0.7227815063384185, F1-Score: 0.8261240139997109
Correction Precision: 0.9649253731340883, Recall: 0.7226154992547088, F1-Score: 0.8263740940989694
Correction Precision: 0.9652260307995616, Recall: 0.7245944434084048, F1-Score: 0.8277771855787666
Correction Precision: 0.965217391304108, Recall: 0.7242729306486345, F1-Score: 0.827564170348208
Correction Precision: 0.9647292598109874, Recall: 0.7236817588968281, F1-Score: 0.8269988284251644
Epoch 13 Training: 100% 250/250 [03:45<00:00,  1.11it/s, loss=0.00526, c_precision=0.991, c_recall=0.88, c_f1_score=0.932]
Correction Precision: 0.964864191377781, Recall: 0.7227926078027398, F1-Score: 0.8264674488164072
Eval情况：没降没升
Character-level Detect Acc: 0.9863, P: 0.7375, R: 0.5446, F1: 0.6265
Character-level Correct Acc: 0.9837, P: 0.6851, R: 0.4215, F1: 0.5219
Sentence-level Detect Acc: 0.6855, P: 0.7835, R: 0.5046, F1: 0.6138
Sentence-level Correct Acc: 0.6264, P: 0.7343, R: 0.3853, F1: 0.5054
Detection Eval情况：几乎没差别
Detection Character-level Precision 0.5670103092776199, Recall 0.6223479490797421, F1_Score 0.593391772932514