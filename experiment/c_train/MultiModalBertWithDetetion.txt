# FIXME d_loss忘了对pad进行mask了
模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。Detection模块的head也是macbert的head，detection模块在head前一层把特征加到correction模块中去。d_loss和loss分别是0.3,0.7。
代码：MultiModalBertWithDetection1.py
执行情况：第14个epoch我手动停了，看起来不会再增长了，虽然还没有过拟合训练集。效果比之前要好1个点，但并没有好太多。
Correction Precision: 0.4484823456534279, Recall: 0.40712277413300707, F1-Score: 0.42680290773452917
Correction Precision: 0.8068241469814156, Recall: 0.5764110256889975, F1-Score: 0.6724269928419558
Correction Precision: 0.8784835551089936, Recall: 0.6570892018778108, F1-Score: 0.75182638541483
Correction Precision: 0.9105275954289057, Recall: 0.7010482965180267, F1-Score: 0.7921734527076457
Correction Precision: 0.9254163649527092, Recall: 0.7194595608930907, F1-Score: 0.8095439184265869
Correction Precision: 0.9335225928552321, Recall: 0.7403377110692794, F1-Score: 0.8257821487164902
Correction Precision: 0.9367058823527207, Recall: 0.7457849381789535, F1-Score: 0.8304130157766029
Correction Precision: 0.9383208255157273, Recall: 0.7507975229872863, F1-Score: 0.8341499004754453
Correction Precision: 0.9380717804360923, Recall: 0.7494377811093048, F1-Score: 0.8332117924011672
Correction Precision: 0.9387277829745231, Recall: 0.7525309336331547, F1-Score: 0.8353798122009956
Correction Precision: 0.938713450292178, Recall: 0.7519205546185587, F1-Score: 0.8349979187735119
Correction Precision: 0.9391243268553173, Recall: 0.7518275538892686, F1-Score: 0.8351030600930934
Correction Precision: 0.9382311651846189, Recall: 0.7513584410716223, F1-Score: 0.834460513500324
Epoch 13 Training: 100% 250/250 [03:42<00:00,  1.12it/s, loss=0.00926, c_precision=0.991, c_recall=0.882, c_f1_score=0.933]
Correction Precision: 0.9379971923254706, Recall: 0.7517344834050718, F1-Score: 0.834599770506214
Eval情况：eval情况也有提升，大概3个点，但是依然不明显。
Character-level Detect Acc: 0.9836, P: 0.6266, R: 0.5460, F1: 0.5835
Character-level Correct Acc: 0.9813, P: 0.5749, R: 0.4399, F1: 0.4984
Sentence-level Detect Acc: 0.6445, P: 0.7115, R: 0.4752, F1: 0.5699
Sentence-level Correct Acc: 0.6018, P: 0.6688, R: 0.3890, F1: 0.4919

# FIXME d_loss忘了对pad进行mask了
模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。Detection模块的head也是macbert的head，detection模块在head前一层把特征加到correction模块中去。d_loss和loss分别是0.3,0.7。
调整为：0.15的loss和0.75的soft_loss
执行情况：第9个epoch我手动停了，看起来不会再增长了，虽然还没有过拟合训练集。效果不如0.99soft_loss好
Correction Precision: 0.4555620953501166, Recall: 0.4352389878162258, F1-Score: 0.44516871115661655
Correction Precision: 0.8037525354967536, Recall: 0.594412150759311, F1-Score: 0.683410584829591
Correction Precision: 0.8753970193010322, Recall: 0.6728638497651318, F1-Score: 0.7608834142461307
Correction Precision: 0.9041816009555783, Recall: 0.7083489329837686, F1-Score: 0.7943738842558166
Correction Precision: 0.9223462360482254, Recall: 0.7288421842745864, F1-Score: 0.8142557647058574
Correction Precision: 0.9259522119704456, Recall: 0.7343339587240648, F1-Score: 0.8190854865840234
Correction Precision: 0.9290261730721694, Recall: 0.7381041588608583, F1-Score: 0.8226328421830743
Correction Precision: 0.9301940369141196, Recall: 0.7376618502531923, F1-Score: 0.8228152794646276
Epoch 8 Training: 100% 250/250 [03:36<00:00,  1.15it/s, loss=0.00624, c_precision=0.986, c_recall=0.847, c_f1_score=0.911]
Correction Precision: 0.9292118924018571, Recall: 0.7380059970013609, F1-Score: 0.8226446621343552
Eval情况：eval情况也变差了，看来单独计算纠错loss要慎重
Character-level Detect Acc: 0.9816, P: 0.5680, R: 0.5318, F1: 0.5493
Character-level Correct Acc: 0.9793, P: 0.5103, R: 0.4215, F1: 0.4617
Sentence-level Detect Acc: 0.6109, P: 0.6585, R: 0.4459, F1: 0.5317
Sentence-level Correct Acc: 0.5673, P: 0.6075, R: 0.3578, F1: 0.4503
Detection Eval情况：看起来Detection模块是有被训练的
Detection Character-level Precision 0.5966101694905143, Recall 0.49787835926379365, F1_Score 0.5427910557869634

# FIXME d_loss忘了对pad进行mask了
模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。Detection模块的head也是macbert的head。d_loss和loss分别是0.3,0.7。
相比相加融合，本次使用了concat进行融合（因为concat了，correction的head的网络会变宽）。
执行情况：执行了10个epoch后手动停了，很难再涨了。效果相比相加融合差了0.5个点
Correction Precision: 0.7256157635466193, Recall: 0.5529279279278241, F1-Score: 0.6276097140381738
Correction Precision: 0.8818204676889912, Recall: 0.6582207207205971, F1-Score: 0.7537882853781911
Correction Precision: 0.9210718635807743, Recall: 0.7087160262416665, F1-Score: 0.8010593215422169
Correction Precision: 0.9278375149340673, Recall: 0.7281080067502853, F1-Score: 0.8159277154138053
Correction Precision: 0.9314649081486265, Recall: 0.7423048048046654, F1-Score: 0.8261959468636398
Correction Precision: 0.9310183012667923, Recall: 0.7443256424684404, F1-Score: 0.8272698837966649
Correction Precision: 0.9331456720617092, Recall: 0.7455022488754225, F1-Score: 0.8288363366245247
Correction Precision: 0.9333958724200436, Recall: 0.7458770614691255, F1-Score: 0.8291666661727284
Correction Precision: 0.9344454887215848, Recall: 0.7458739684919831, F1-Score: 0.8295786394727489
Epoch 9 Training: 100% 250/250 [03:56<00:00,  1.06it/s, loss=0.00626, c_precision=0.995, c_recall=0.924, c_f1_score=0.958]
Correction Precision: 0.9338494018294783, Recall: 0.7462043111526249, F1-Score: 0.8295478219690776
Eval情况：Eval情况也不如相加融合
Character-level Detect Acc: 0.9835, P: 0.6226, R: 0.5530, F1: 0.5858
Character-level Correct Acc: 0.9808, P: 0.5587, R: 0.4243, F1: 0.4823
Sentence-level Detect Acc: 0.6336, P: 0.6983, R: 0.4587, F1: 0.5537
Sentence-level Correct Acc: 0.5836, P: 0.6436, R: 0.3578, F1: 0.4599
Detection Eval情况：看起来Detection模块是有被训练的，但效果不够好。
Detection Character-level Precision 0.5982608695641769, Recall 0.48656294200779837, F1_Score 0.5366614659631219


模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。加入了Detection模块，Detection模块。Detection模块的head也是macbert的head，detection模块在head前一层把特征加到correction模块中去。d_loss和loss分别是0.3,0.7。
将Detection的head的激活函数从GELU改为了Tanh。
执行情况：13个epoch后停了。效果不如GELU好。
Correction Precision: 0.45574582215794185, Recall: 0.4140581068415343, F1-Score: 0.43390296551952723
Correction Precision: 0.8093487394955858, Recall: 0.5779111194448568, F1-Score: 0.6743244716721759
Correction Precision: 0.8814328960643589, Recall: 0.6561502347416608, F1-Score: 0.7522876515721499
Correction Precision: 0.9117073170729483, Recall: 0.6997379258703295, F1-Score: 0.7917814017537705
Correction Precision: 0.9258724428397289, Recall: 0.7218990429722796, F1-Score: 0.8112610707854322
Correction Precision: 0.930472308798626, Recall: 0.7281425891180622, F1-Score: 0.8169666345983114
Correction Precision: 0.9325681492106808, Recall: 0.7306107156236922, F1-Score: 0.8193277305996371
Correction Precision: 0.9332376166544787, Recall: 0.731844623756665, F1-Score: 0.8203618000961195
Correction Precision: 0.9330143540667624, Recall: 0.7308845577210024, F1-Score: 0.8196721306547473
Correction Precision: 0.9336357125803452, Recall: 0.7332208473939382, F1-Score: 0.8213798167915247
Correction Precision: 0.9333333333331102, Recall: 0.7318718381111613, F1-Score: 0.8204158785241592
Correction Precision: 0.9340028694402357, Recall: 0.7321462043110155, F1-Score: 0.8208469050446255
Epoch 12 Training: 100% 250/250 [03:33<00:00,  1.17it/s, loss=0.0215, c_precision=0.986, c_recall=0.82, c_f1_score=0.895]
Correction Precision: 0.93307839388123, Recall: 0.731497095746537, F1-Score: 0.8200819236747898
Eval情况：不太行
Character-level Detect Acc: 0.9822, P: 0.5864, R: 0.5233, F1: 0.5531
Character-level Correct Acc: 0.9798, P: 0.5272, R: 0.4116, F1: 0.4623
Sentence-level Detect Acc: 0.6218, P: 0.6817, R: 0.4440, F1: 0.5378
Sentence-level Correct Acc: 0.5764, P: 0.6295, R: 0.3523, F1: 0.4518
Detection Eval情况：
Detection Character-level Precision 0.5947281713334519, Recall 0.5106082036767884, F1_Score 0.5494672749967322