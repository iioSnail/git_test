模型：MultiModalBert，随便弄的，glyph和pinyin信息大概都是75%。
执行情况：花费2个多小时
Correction Precision: 0.0787985865724312, Recall: 0.16484938089074758, F1-Score: 0.10662841388307617
Correction Precision: 0.27237354085598064, Recall: 0.2715684463328521, F1-Score: 0.2719703972797843
Correction Precision: 0.40982763056331106, Recall: 0.2948362021098843, F1-Score: 0.34294940747879543
Correction Precision: 0.5084544645504276, Recall: 0.31693786982242655, F1-Score: 0.39047727484292544
Correction Precision: 0.5908529048205838, Recall: 0.3529628945910369, F1-Score: 0.4419276546801662
Correction Precision: 0.6726984126981992, Recall: 0.39153732446408135, F1-Score: 0.4949778084579314
Correction Precision: 0.7430615164518618, Recall: 0.47888622533644126, F1-Score: 0.5824175819408212
Correction Precision: 0.7909482758618559, Recall: 0.5426987060997148, F1-Score: 0.6437184823060457
Correction Precision: 0.8163057324838685, Recall: 0.5930038867294849, F1-Score: 0.6869639789292151
Correction Precision: 0.8420792079205837, Recall: 0.6290680473371617, F1-Score: 0.7201524127194746
Correction Precision: 0.8498334126604354, Recall: 0.6611738566930825, F1-Score: 0.7437259184912905
Correction Precision: 0.8599203560550808, Recall: 0.6779316712833466, F1-Score: 0.7581577855456724
Correction Precision: 0.8661710037172707, Recall: 0.6902425476762284, F1-Score: 0.768263781062129
Correction Precision: 0.8772663877264347, Recall: 0.6975970425137342, F1-Score: 0.7771828660631845
Correction Precision: 0.8764786169242773, Recall: 0.7119364375460621, F1-Score: 0.7856851544807353
Correction Precision: 0.8798098687186781, Recall: 0.7183515061909594, F1-Score: 0.790924813829899
Correction Precision: 0.886018581463656, Recall: 0.7231366746808353, F1-Score: 0.7963340117249199
Correction Precision: 0.8848728246316633, Recall: 0.7330868761551325, F1-Score: 0.8018600884650147
Correction Precision: 0.8909984364527828, Recall: 0.7376109467454257, F1-Score: 0.8070814360242516
Correction Precision: 0.8914067758580005, Recall: 0.7350601295095772, F1-Score: 0.8057189206157932


模型：MultiModalBert，和上面模型一样，但0.7*错字loss+0.3*复制字loss，相当于把侧重点放在改错字上
执行情况：召回率高，但精准率低。这是正常，因为模型不擅长复制原有字，而更偏向于改错，而且精准率还在不断增高。一共跑了30个epoch，这个方法有潜力。
Correction Precision: 0.02612711333750742, Recall: 0.30844575863975077, F1-Score: 0.048173644340866836
Correction Precision: 0.07302671033814474, Recall: 0.5848882320338842, F1-Score: 0.12984190108834653
Correction Precision: 0.11245818114851869, Recall: 0.678141773088899, F1-Score: 0.19292333589750169
Correction Precision: 0.14544098452358212, Recall: 0.7211538461537127, F1-Score: 0.24206312233493993
Correction Precision: 0.18556654061687614, Recall: 0.7518921912496304, F1-Score: 0.29766863960059764
Correction Precision: 0.22424209437760712, Recall: 0.7612712490759864, F1-Score: 0.3464368295832744
Correction Precision: 0.2640586797065849, Recall: 0.7766918679696152, F1-Score: 0.3941237013149178
Correction Precision: 0.2983320430712718, Recall: 0.7835489833639956, F1-Score: 0.4321321164260555
Correction Precision: 0.3451916829109531, Recall: 0.7866000370163266, F1-Score: 0.47981936169849404
Correction Precision: 0.37759663423609624, Recall: 0.7965976331359473, F1-Score: 0.5123387044455715
Correction Precision: 0.40329989687818474, Recall: 0.7965191631177936, F1-Score: 0.5354742340569981
Correction Precision: 0.44594177553745024, Recall: 0.8005540166203508, F1-Score: 0.5728065534515958
Correction Precision: 0.4702497285558664, Recall: 0.8018885391592664, F1-Score: 0.5928410097316956
Correction Precision: 0.49418604651157155, Recall: 0.8012939001846947, F1-Score: 0.6113383157034463
Correction Precision: 0.5258631132646243, Recall: 0.8021064301550623, F1-Score: 0.6352527982337695
Correction Precision: 0.5544352265474722, Recall: 0.8028090925890218, F1-Score: 0.6558961190999155
Correction Precision: 0.577132245655846, Recall: 0.804697614203661, F1-Score: 0.6721767336398277
Correction Precision: 0.6048699501936075, Recall: 0.8081330868760058, F1-Score: 0.6918816263499268
Correction Precision: 0.6271601382487576, Recall: 0.8052884615383126, F1-Score: 0.7051489632381874
Correction Precision: 0.6476984954564803, Recall: 0.804440333024828, F1-Score: 0.7176101661998809
Correction Precision: 0.6721109399074465, Recall: 0.8056889545620971, F1-Score: 0.7328629027297684
Correction Precision: 0.690891780387069, Recall: 0.8046571798187386, F1-Score: 0.7434474510523266
Correction Precision: 0.7055690072638086, Recall: 0.8072022160663329, F1-Score: 0.7529715757295172
Correction Precision: 0.7265611990007116, Recall: 0.8057248384116702, F1-Score: 0.7640980730563671
Correction Precision: 0.7463867859599541, Recall: 0.8025901942644212, F1-Score: 0.7734688414368668
Correction Precision: 0.7512454904654267, Recall: 0.8075715604799986, F1-Score: 0.7783908859369686
Correction Precision: 0.7648092810685946, Recall: 0.8041027536498236, F1-Score: 0.7839639634641362
Correction Precision: 0.7801075268815806, Recall: 0.8038781163433418, F1-Score: 0.7918144606186588
Correction Precision: 0.7958471150310922, Recall: 0.8001108442636615, F1-Score: 0.7979732837006855
Correction Precision: 0.8042632066726961, Recall: 0.8024782689105229, F1-Score: 0.8033697458431216

模型：MultiModalBert，将PinyinEmbeddings换成了手动Embedding（就是a=1,b=2这样），这样效果居然是最好的，中间不需要学习，而且在CSC任务上也表现更好。参数少了反而结果好了
执行情况：
Correction Precision: 0.07042019598072825, Recall: 0.15619819487932285, F1-Score: 0.09707515268249438
Correction Precision: 0.2600668190609706, Recall: 0.27302935203982404, F1-Score: 0.2663904894137923
Correction Precision: 0.40020237794070324, Recall: 0.2916666666666129, F1-Score: 0.3374213496242088
Correction Precision: 0.500737245650103, Recall: 0.31270718232038436, F1-Score: 0.3849903634309478
Correction Precision: 0.5763734518893057, Recall: 0.33468559837722023, F1-Score: 0.423471768083852
Correction Precision: 0.6475384132955009, Recall: 0.3806451612902524, F1-Score: 0.47945205432809934
Correction Precision: 0.7162503508277529, Recall: 0.47032805012892176, F1-Score: 0.5678050946376038
Correction Precision: 0.7764768493877657, Recall: 0.5375829034634234, F1-Score: 0.6353146087042816
Correction Precision: 0.8111111111109063, Recall: 0.5920737327187848, F1-Score: 0.6844965365392087
Correction Precision: 0.824282665401749, Recall: 0.642039157739076, F1-Score: 0.7218357382678269
Correction Precision: 0.8403361344535852, Recall: 0.6628613515005224, F1-Score: 0.7411219758320872
Correction Precision: 0.8546049555865229, Recall: 0.6734205194325523, F1-Score: 0.7532708349864256
Correction Precision: 0.8576642335764466, Recall: 0.6932153392329105, F1-Score: 0.7667210435511423
Correction Precision: 0.855849889624535, Recall: 0.7138648499354237, F1-Score: 0.7784358995139736
Correction Precision: 0.8692152917503086, Recall: 0.7172108467071172, F1-Score: 0.7859308666966691
Correction Precision: 0.8764019739791663, Recall: 0.7189915347808761, F1-Score: 0.789931257086592
Correction Precision: 0.8738079396760092, Recall: 0.7266691257837096, F1-Score: 0.7934749768477375
Correction Precision: 0.8858744394616846, Recall: 0.7278924097272056, F1-Score: 0.7991504849415232
Correction Precision: 0.8840035351301625, Recall: 0.7375115207371912, F1-Score: 0.8041402869122198
Correction Precision: 0.8906284454242798, Recall: 0.7446533923302461, F1-Score: 0.8111256145254097

模型：MultiModalBert，Manual Pinyin Embedding，Dense Glyph Embeddings, 但是Dense Glyph Embeddings网络是没有经过训练的。
执行情况：最终效果并不好，看来还是需要对Glyph Embedding层进行预训练才行。这么说网络在做预测时确实使用了Glyph信息。
补充：好像不是这个原因，因为我上面MultiModalBert的结果复现不出来了，所以可能是代码某个地方出现了问题
Correction Precision: 0.05667541074140158, Recall: 0.15122490329708024, F1-Score: 0.082450413864013
Correction Precision: 0.18047263681589795, Recall: 0.2667769810626463, F1-Score: 0.21529787028051212
Correction Precision: 0.2529170608640408, Recall: 0.2947445791987698, F1-Score: 0.27223353650219145
Correction Precision: 0.3028283192217073, Recall: 0.30929162833480967, F1-Score: 0.30602585058319687
Correction Precision: 0.34616843702573097, Recall: 0.3354162837713039, F1-Score: 0.34070755107291617
Correction Precision: 0.37673509286405144, Recall: 0.35494566218449897, F1-Score: 0.365515932732544
Correction Precision: 0.4165238911097627, Recall: 0.4028723991897619, F1-Score: 0.4095844248089352
Correction Precision: 0.4704508712052325, Recall: 0.48152233866510724, F1-Score: 0.47592222374130405
Correction Precision: 0.5097283085012253, Recall: 0.534362366776822, F1-Score: 0.5217547317151303
Correction Precision: 0.5363345135726628, Recall: 0.584742647058716, F1-Score: 0.5594934477463706
Correction Precision: 0.5559788009273673, Recall: 0.6173225450532148, F1-Score: 0.5850470542241647
Correction Precision: 0.570827943078821, Recall: 0.6502118253820869, F1-Score: 0.6079393777849673
Correction Precision: 0.5753577106517368, Recall: 0.665379665379543, F1-Score: 0.6171029068262543
Correction Precision: 0.5855086220533798, Recall: 0.6800808526275853, F1-Score: 0.6292612423829025
Correction Precision: 0.5900630914825568, Recall: 0.6883164673411797, F1-Score: 0.6354140122416991
Correction Precision: 0.5927204267335122, Recall: 0.694357654842732, F1-Score: 0.6395260257408428
Correction Precision: 0.6005391690452584, Recall: 0.6975501934056552, F1-Score: 0.6454196842065597
Correction Precision: 0.6011288805267166, Recall: 0.7059473393480563, F1-Score: 0.649335252276414
Correction Precision: 0.6064930991216112, Recall: 0.7109762824047231, F1-Score: 0.6545916203240241
Correction Precision: 0.6099235450147277, Recall: 0.7183020948179495, F1-Score: 0.6596911648057237


模型：MultiModalBert，最后的cls层使用了3层的前馈神经网络。不更新bert的权重，只更新cls的权重
执行情况：学习速度偏慢，但是能学会，到第20次的时候还在增长。到40次，f1到48时就出现瓶颈了
Correction Precision: 0.008080127935358807, Recall: 0.07181597157283115, F1-Score: 0.014525921400909513
Correction Precision: 0.02605912257578512, Recall: 0.12936997569636766, F1-Score: 0.04338014014216185
Correction Precision: 0.06378037160552263, Recall: 0.20037418147797934, F1-Score: 0.09676107837064266
Correction Precision: 0.11207194742302731, Recall: 0.24251497005983483, F1-Score: 0.15330021248460957
Correction Precision: 0.17475394827189575, Recall: 0.2859550561797217, F1-Score: 0.2169342231404072
Correction Precision: 0.24415903811992837, Recall: 0.334143605085951, F1-Score: 0.2821504692365423
Correction Precision: 0.3196187450356919, Recall: 0.37642656688486875, F1-Score: 0.345704466857223
Correction Precision: 0.38469694292270984, Recall: 0.4072965388212521, F1-Score: 0.3956742997547881
Correction Precision: 0.4555619818776835, Recall: 0.44242651188907645, F1-Score: 0.448898175791815
Correction Precision: 0.5116422831237355, Recall: 0.4644594089037664, F1-Score: 0.48691048092081984
Correction Precision: 0.5555317783007584, Recall: 0.48568755846576506, F1-Score: 0.5182671186876124
Correction Precision: 0.5985742927153934, Recall: 0.5026187803964641, F1-Score: 0.5464158612220381
Correction Precision: 0.639269406392548, Recall: 0.5242463958059307, F1-Score: 0.5760724200376349
Correction Precision: 0.6728164409153029, Recall: 0.5395131086141312, F1-Score: 0.5988360003373486
Correction Precision: 0.689922480619993, Recall: 0.5499999999998969, F1-Score: 0.6120662702158364
Correction Precision: 0.7267139479903718, Recall: 0.5752245508980959, F1-Score: 0.6421558382365287
Correction Precision: 0.749217809867449, Recall: 0.5823045267488622, F1-Score: 0.6552994416717016
Correction Precision: 0.765828902851434, Recall: 0.5931124836233215, F1-Score: 0.6684948840137894
Correction Precision: 0.781679764243423, Recall: 0.5957327344187543, F1-Score: 0.6761550712030736
Correction Precision: 0.794430992735885, Recall: 0.613615111277237, F1-Score: 0.6924132104398286
再学20次：
Correction Precision: 0.8120740019472219, Recall: 0.6239012530389706, F1-Score: 0.7056583813170033
Correction Precision: 0.81771720613268, Recall: 0.6281547952887215, F1-Score: 0.7105096209929016
Correction Precision: 0.8236580034003342, Recall: 0.6344246959774303, F1-Score: 0.7167617834862875
Correction Precision: 0.836779372415267, Recall: 0.6437125748501789, F1-Score: 0.7276573236754563
Correction Precision: 0.840309702395151, Recall: 0.6503745318350841, F1-Score: 0.733241844752341
Correction Precision: 0.8455971049455137, Recall: 0.6553851907253823, F1-Score: 0.7384388491865846
Correction Precision: 0.853188929000998, Recall: 0.6632366697847215, F1-Score: 0.7463157889813723
Correction Precision: 0.8585907335905263, Recall: 0.6656688493918305, F1-Score: 0.7499209606207189
Correction Precision: 0.860687022900558, Recall: 0.6755289271670706, F1-Score: 0.7569495431974435
Correction Precision: 0.861961722487832, Recall: 0.6739618406283812, F1-Score: 0.7564560146238554
Correction Precision: 0.8667944417822551, Recall: 0.6768942937323336, F1-Score: 0.7601638822682009
Correction Precision: 0.8672714251609291, Recall: 0.6795735129067191, F1-Score: 0.7620346088412346
Correction Precision: 0.8678885979526617, Recall: 0.6826436996815797, F1-Score: 0.7642003767863729
Correction Precision: 0.8730385164049279, Recall: 0.6876404494380735, F1-Score: 0.7693274665087804
Correction Precision: 0.8734356552536308, Recall: 0.6926966292133534, F1-Score: 0.7726370752245133
Correction Precision: 0.8795352146073322, Recall: 0.6940494011974748, F1-Score: 0.7758602651693725
Correction Precision: 0.879461120302321, Recall: 0.6960344182565102, F1-Score: 0.7770700632008825
Correction Precision: 0.8794359576966205, Recall: 0.7003556054649633, F1-Score: 0.779745779877173
Correction Precision: 0.8804143126174951, Recall: 0.6999812839227587, F1-Score: 0.7798978203800905
Correction Precision: 0.8796816479398689, Recall: 0.7028240134653632, F1-Score: 0.7813702043090763
Correction Precision: 0.8834427767352524, Recall: 0.7045072002991014, F1-Score: 0.7838934549218551
Correction Precision: 0.8850198644541049, Recall: 0.7083800972688536, F1-Score: 0.7869090904150721

模型：MultiModalBert，最后的cls层使用了2层的Transformer。不更新bert的权重，只更新cls的权重
执行情况：比3层Dense层好，训练到40个epoch后效果和之前的差不都，虽然还在涨，但感觉涨不太动了。
Correction Precision: 0.0038887773172912956, Recall: 0.039335180055394395, F1-Score: 0.007077822658077036
Correction Precision: 0.024105629975382074, Recall: 0.13921713441651787, F1-Score: 0.041095516848798676
Correction Precision: 0.057618497109823925, Recall: 0.22993172171983206, F1-Score: 0.09214613190850107
Correction Precision: 0.09968827930173942, Recall: 0.29512735326683365, F1-Score: 0.14903532444308504
Correction Precision: 0.15292973498661536, Recall: 0.3481549815497512, F1-Score: 0.21251196533971026
Correction Precision: 0.2155562255049527, Recall: 0.3959025470652647, F1-Score: 0.2791333198635847
Correction Precision: 0.2769830949284458, Recall: 0.43260709010331744, F1-Score: 0.3377297292537307
Correction Precision: 0.3364661654134886, Recall: 0.4632162661736666, F1-Score: 0.3897962353190023
Correction Precision: 0.389415950192649, Recall: 0.4844182186980482, F1-Score: 0.43175281403455945
Correction Precision: 0.4398774983880658, Recall: 0.5035984498984123, F1-Score: 0.46958616486398264
Correction Precision: 0.48995708154498024, Recall: 0.527541589648701, F1-Score: 0.5080551841912904
Correction Precision: 0.5364394488758998, Recall: 0.5458402508761213, F1-Score: 0.5410990211695008
Correction Precision: 0.5791258477768313, Recall: 0.5675775480058036, F1-Score: 0.5732935466838304
Correction Precision: 0.6156694601441014, Recall: 0.5834872552640222, F1-Score: 0.599146514436235
Correction Precision: 0.6457336523124709, Recall: 0.5981532779315608, F1-Score: 0.6210334574624569
Correction Precision: 0.6777755060313477, Recall: 0.6121883656508564, F1-Score: 0.6433145735357106
Correction Precision: 0.6961712638944635, Recall: 0.6239852398522834, F1-Score: 0.6581046891296991
Correction Precision: 0.7156249999998509, Recall: 0.6338807898135017, F1-Score: 0.6722771303364224
Correction Precision: 0.7300420168065692, Recall: 0.6417359187441104, F1-Score: 0.6830466825486207
Correction Precision: 0.7457088366177694, Recall: 0.6499815293681843, F1-Score: 0.6945623206311995
Correction Precision: 0.7645429362879257, Recall: 0.6628486975797777, F1-Score: 0.7100732233298266
Correction Precision: 0.7773479475605464, Recall: 0.6679593721143734, F1-Score: 0.7185141035948945
Correction Precision: 0.7924691625187638, Recall: 0.6757704373499398, F1-Score: 0.7294820712161606
Correction Precision: 0.7993506493504763, Recall: 0.6811139800810252, F1-Score: 0.7355108539145143
Correction Precision: 0.8087277464175404, Recall: 0.6875230712438746, F1-Score: 0.7432162804289109
Correction Precision: 0.8178058336959473, Recall: 0.6931734317341894, F1-Score: 0.7503495101882929
Correction Precision: 0.8325285338013976, Recall: 0.6999446392322015, F1-Score: 0.7605012526364222
Correction Precision: 0.8353792196403254, Recall: 0.703915773919338, F1-Score: 0.7640336803374954
Correction Precision: 0.8404022737208481, Recall: 0.7096178696694277, F1-Score: 0.7694925427923656
Correction Precision: 0.8480349344976313, Recall: 0.7170020306441356, F1-Score: 0.7770331094363292
Correction Precision: 0.8517626450622177, Recall: 0.7177121771216387, F1-Score: 0.7790127160349823
Correction Precision: 0.8564530289725951, Recall: 0.7205909510617321, F1-Score: 0.7826697417560863
Correction Precision: 0.8593030900721325, Recall: 0.722631772944946, F1-Score: 0.7850635694304944
Correction Precision: 0.8635164835162936, Recall: 0.7245067305917896, F1-Score: 0.7879274034944482
Correction Precision: 0.8666959000217349, Recall: 0.7290667650312193, F1-Score: 0.7919463082283926
Correction Precision: 0.8686403508770024, Recall: 0.7310815799186542, F1-Score: 0.793946682205482
Correction Precision: 0.8735404274067253, Recall: 0.7318198597267014, F1-Score: 0.7964246253950694
Correction Precision: 0.8739864124477593, Recall: 0.736608792020551, F1-Score: 0.7994387085342481
Correction Precision: 0.8758741258739344, Recall: 0.7404396822463069, F1-Score: 0.8024827305074542
Correction Precision: 0.8809732573430773, Recall: 0.7411027106766105, F1-Score: 0.8050075107704568
Correction Precision: 0.8807017543857717, Recall: 0.7420546932740683, F1-Score: 0.8054552742728425
Correction Precision: 0.8846238694019667, Recall: 0.7406723309935831, F1-Score: 0.8062732477193024

模型：MultiModalBert，使用macbert作为backbone，macbert的head。0.25的loss和0.75的soft_loss
执行情况：第13个epoch就early stop了，训练数据差不多可以完全拟合
Correction Precision: 0.5767513471900574, Recall: 0.6990110095165704, F1-Score: 0.6320229453455516
Correction Precision: 0.6840418353575094, Recall: 0.755965697240724, F1-Score: 0.7182075800891469
Correction Precision: 0.7334382101030006, Recall: 0.7838595180271279, F1-Score: 0.7578110885378
Correction Precision: 0.7826656682888837, Recall: 0.7803284807762634, F1-Score: 0.7814953266026586
Correction Precision: 0.8127809263238591, Recall: 0.7765123226286824, F1-Score: 0.794232788576782
Correction Precision: 0.8349514563105107, Recall: 0.7697184411708428, F1-Score: 0.8010090224947524
Correction Precision: 0.882488963632356, Recall: 0.7832089552237345, F1-Score: 0.8298902831825451
Correction Precision: 0.9011652999566462, Recall: 0.7793952967523741, F1-Score: 0.8358686944584225
Correction Precision: 0.902780771717848, Recall: 0.7816349384097084, F1-Score: 0.8378513549090406
Correction Precision: 0.9034557235419214, Recall: 0.7795378307862878, F1-Score: 0.8369347734121069
Correction Precision: 0.9037629757783512, Recall: 0.7792280440050756, F1-Score: 0.8368879538432128
Correction Precision: 0.9027178602241365, Recall: 0.780055917986807, F1-Score: 0.8369163078716523
Correction Precision: 0.9040622299047311, Recall: 0.7801603580084318, F1-Score: 0.8375537979211151
Epoch 13 Training: 100% 250/250 [03:02<00:00,  1.37it/s, loss=0.00329, c_precision=0.996, c_recall=0.999, c_f1_score=0.998]
Correction Precision: 0.903434867141736, Recall: 0.7793514722324302, F1-Score: 0.8368184087071534

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重
执行情况：12个epoch就early stop了。训练数据差不多可以完全拟合
Correction Precision: 0.5514391564546733, Recall: 0.361074827393103, F1-Score: 0.436400540793651
Correction Precision: 0.8061327927012656, Recall: 0.5930275913496283, F1-Score: 0.6833512347423867
Correction Precision: 0.8396048918154186, Recall: 0.6669157481784668, F1-Score: 0.7433628313648218
Correction Precision: 0.8557303370784592, Recall: 0.7107129525941189, F1-Score: 0.7765089717716637
Correction Precision: 0.9154539154537039, Recall: 0.7399178491410119, F1-Score: 0.8183789360051951
Correction Precision: 0.9182681176196067, Recall: 0.7395114674621033, F1-Score: 0.8192522201212195
Correction Precision: 0.9211799953904306, Recall: 0.7457089552237414, F1-Score: 0.8242086808127218
Correction Precision: 0.9244845957838951, Recall: 0.7448674878684686, F1-Score: 0.8250129194022586
Correction Precision: 0.924663885025284, Recall: 0.7444942142589128, F1-Score: 0.824855251780262
Correction Precision: 0.9230235783631707, Recall: 0.7441297055533461, F1-Score: 0.8239785385068251
Correction Precision: 0.923432801295183, Recall: 0.7443595002795553, F1-Score: 0.8242824690492652
Correction Precision: 0.9235389235387101, Recall: 0.7452003727864407, F1-Score: 0.8248401067883957
Epoch 12 Training: 100% 250/250 [03:20<00:00,  1.25it/s, loss=0.00252, c_precision=0.998, c_recall=0.989, c_f1_score=0.993]
Correction Precision: 0.9238954429791987, Recall: 0.7447324258808978, F1-Score: 0.8246954362183752
Eval情况：
Character-level Detect Acc: 0.9748, P: 0.4110, R: 0.4540, F1: 0.4315
Character-level Correct Acc: 0.9735, P: 0.3775, R: 0.3946, F1: 0.3859
Sentence-level Detect Acc: 0.4891, P: 0.4767, R: 0.3193, F1: 0.3824
Sentence-level Correct Acc: 0.4755, P: 0.4543, R: 0.2917, F1: 0.3553

# FIXME 有问题，实际上是2的copy_loss，代码有bug
模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.25的loss和0.75的soft_loss
执行情况：执行了7个epoch手动停了，基本上已经过拟合训练集了。
Correction Precision: 0.5744434807746244, Recall: 0.3707781302481114, F1-Score: 0.45066908548364637
Correction Precision: 0.8090050835146915, Recall: 0.623042505592725, F1-Score: 0.7039494465856923
Correction Precision: 0.8412338398727962, Recall: 0.69288249579662, F1-Score: 0.7598852689165255
Correction Precision: 0.871522280026946, Recall: 0.719111608809123, F1-Score: 0.7880151339762567
Correction Precision: 0.9187457083998811, Recall: 0.7494398805077017, F1-Score: 0.825501284852024
Correction Precision: 0.9175022789423615, Recall: 0.7506992355023772, F1-Score: 0.8257614598678692
Correction Precision: 0.9252807701120959, Recall: 0.7531716417909042, F1-Score: 0.8304021387625171
Epoch 7 Training: 100% 250/250 [03:29<00:00,  1.19it/s, loss=0.00252, c_precision=0.996, c_recall=0.986, c_f1_score=0.991]
Correction Precision: 0.9253216911762578, Recall: 0.7515864128404718, F1-Score: 0.8294541704629714
Eval情况：
Character-level Detect Acc: 0.9752, P: 0.4223, R: 0.4880, F1: 0.4528
Character-level Correct Acc: 0.9738, P: 0.3886, R: 0.4243, F1: 0.4057
Sentence-level Detect Acc: 0.5155, P: 0.5155, R: 0.3651, F1: 0.4275
Sentence-level Correct Acc: 0.4936, P: 0.4834, R: 0.3211, F1: 0.3859

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。
在此基础上，0.25的loss和0.75的soft_loss。增加0.25的label_smooth
执行情况：执行了16个epoch然后early stop了，基本上已经过拟合训练集了。
**看起来label_smooth过拟合没什么效果**
Correction Precision: 0.5835205992507906, Recall: 0.43608882254143755, F1-Score: 0.499145663756396
Correction Precision: 0.8026375625282395, Recall: 0.6580909768828005, F1-Score: 0.7232124559685873
Correction Precision: 0.8683566822324064, Recall: 0.7294974780495554, F1-Score: 0.7928934005188435
Correction Precision: 0.898992161253998, Recall: 0.7491601343783596, F1-Score: 0.8172656006441469
Correction Precision: 0.912605811027016, Recall: 0.7447722180730498, F1-Score: 0.8201912198195467
Correction Precision: 0.915374087591032, Recall: 0.7482752190936512, F1-Score: 0.8234328506385572
Correction Precision: 0.9153583617745357, Recall: 0.7505597014923973, F1-Score: 0.8248077903811972
Correction Precision: 0.9156599042623852, Recall: 0.7497200447926932, F1-Score: 0.8244227804180838
Correction Precision: 0.9151308304889839, Recall: 0.7506532288165825, F1-Score: 0.8247718645718641
Correction Precision: 0.9153968614962666, Recall: 0.750093179276789, F1-Score: 0.8245416362967262
Correction Precision: 0.9146951774338228, Recall: 0.749766921499021, F1-Score: 0.8240598417016866
Correction Precision: 0.915739268680237, Recall: 0.7515377446410527, F1-Score: 0.8255528250575059
Correction Precision: 0.9164389799633613, Recall: 0.750512772701706, F1-Score: 0.8252178365114479
Correction Precision: 0.9157559198540719, Recall: 0.7495341036152162, F1-Score: 0.8243492514006928
Correction Precision: 0.9154704944176542, Recall: 0.7493472584854999, F1-Score: 0.8241206025198847
Correction Precision: 0.9149954421146501, Recall: 0.7499066118788289, F1-Score: 0.824266063967896
Epoch 16 Training: 100% 250/250 [03:17<00:00,  1.26it/s, loss=0.00509, c_precision=0.99, c_recall=0.954, c_f1_score=0.972]
Correction Precision: 0.9151308304889839, Recall: 0.7502331654540663, F1-Score: 0.8245182446871847
Eval情况：
Character-level Detect Acc: 0.9734, P: 0.3960, R: 0.4979, F1: 0.4411
Character-level Correct Acc: 0.9718, P: 0.3561, R: 0.4201, F1: 0.3855
Sentence-level Detect Acc: 0.4809, P: 0.4695, R: 0.3670, F1: 0.4119
Sentence-level Correct Acc: 0.4582, P: 0.4364, R: 0.3211, F1: 0.3700


FIXME：有问题，忘了Ignore 0 index了
模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.1的loss和0.9的soft_loss
执行情况：13个epoch后early stop，完全拟合训练集。
Correction Precision: 0.4687731424338512, Recall: 0.3543571561857894, F1-Score: 0.4036131769803506
Correction Precision: 0.7464197530862354, Recall: 0.5635719612228628, F1-Score: 0.6422349687044209
Correction Precision: 0.8145421076850403, Recall: 0.6613114141601604, F1-Score: 0.7299721615838041
Correction Precision: 0.8288188277085192, Recall: 0.6967151922357788, F1-Score: 0.7570472515822815
Correction Precision: 0.8664660958109542, Recall: 0.6990291262134617, F1-Score: 0.7737935305585719
Correction Precision: 0.8629893238432244, Recall: 0.7234756666043775, F1-Score: 0.7870980824735822
Correction Precision: 0.9070351758791897, Recall: 0.7408582089550856, F1-Score: 0.8155678779193757
Correction Precision: 0.9192415730334926, Recall: 0.7329227323626851, F1-Score: 0.8155763234937283
Correction Precision: 0.9134149157236403, Recall: 0.7383351997012433, F1-Score: 0.8165961394579724
Correction Precision: 0.9147161066046546, Recall: 0.7355572120758972, F1-Score: 0.8154116305355767
Correction Precision: 0.9147556173266354, Recall: 0.7363415998506924, F1-Score: 0.8159090904147605
Correction Precision: 0.9150266265337079, Recall: 0.7366262814537303, F1-Score: 0.81619165584602
Correction Precision: 0.9158553546590366, Recall: 0.7367145254520349, F1-Score: 0.8165753844390538
Epoch 13 Training: 100% 250/250 [03:27<00:00,  1.20it/s, loss=0.0464, c_precision=0.983, c_recall=0.99, c_f1_score=0.987]
Correction Precision: 0.9151992585725404, Recall: 0.73611628773747, F1-Score: 0.8159471178696862
Eval情况：感觉泛化性还不错
Character-level Detect Acc: 0.9767, P: 0.4511, R: 0.4823, F1: 0.4662
Character-level Correct Acc: 0.9752, P: 0.4122, R: 0.4116, F1: 0.4119
Sentence-level Detect Acc: 0.5064, P: 0.5026, R: 0.3541, F1: 0.4155
Sentence-level Correct Acc: 0.4836, P: 0.4680, R: 0.3083, F1: 0.3717


FIXME：有问题，忘了Ignore 0 index了
模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss
执行情况：在11个Epoch的时候，我手动停了，因为早就过拟合训练数据了，再训练下去也不会有啥太好的效果了。
Correction Precision: 0.5451686678100214, Recall: 0.35584997200963686, F1-Score: 0.4306198482292144
Correction Precision: 0.794760935910276, Recall: 0.5825876211781166, F1-Score: 0.672332185398123
Correction Precision: 0.8350058343055227, Recall: 0.6684102372500151, F1-Score: 0.7424776919733023
Correction Precision: 0.8518183871694484, Recall: 0.7038073908173378, F1-Score: 0.7707715886714612
Correction Precision: 0.8902906059921399, Recall: 0.7378640776697651, F1-Score: 0.8069423170131504
Correction Precision: 0.9150356896152624, Recall: 0.7410031698674732, F1-Score: 0.8188749222335624
Correction Precision: 0.9203132197142053, Recall: 0.7455223880595624, F1-Score: 0.823747680395873
Correction Precision: 0.9233433387206365, Recall: 0.7463605823066916, F1-Score: 0.8254721844574547
Correction Precision: 0.9237875288681469, Recall: 0.7465472191114694, F1-Score: 0.8257638310496375
Correction Precision: 0.9234140715107442, Recall: 0.745993291091922, F1-Score: 0.8252757442740577
Correction Precision: 0.9231833910032472, Recall: 0.7462241282862676, F1-Score: 0.8253248087444661
Epoch 11 Training: 100% 250/250 [03:27<00:00,  1.20it/s, loss=0.0128, c_precision=0.998, c_recall=0.982, c_f1_score=0.99]
Correction Precision: 0.9237151417375146, Recall: 0.747064305684856, F1-Score: 0.8260511124485352
Eval情况：
Character-level Detect Acc: 0.9772, P: 0.4590, R: 0.4668, F1: 0.4628
Character-level Correct Acc: 0.9759, P: 0.4246, R: 0.4059, F1: 0.4150
Sentence-level Detect Acc: 0.5182, P: 0.5209, R: 0.3431, F1: 0.4137
Sentence-level Correct Acc: 0.5000, P: 0.4926, R: 0.3064, F1: 0.3778

模型：MultiModalBert，使用macbert作为backbone，macbert的head。
加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的copy_loss。使用2e-6学习率训练bert，2e-4训练cls
执行情况：训练了11个epoch手动停了，看起来训练集很难过拟合了，可能是模型还不够复杂，也许这样也好。
Correction Precision: 0.43727087576365836, Recall: 0.4006344467250605, F1-Score: 0.41815171826609016
Correction Precision: 0.7895285230526727, Recall: 0.5650633855330788, F1-Score: 0.6586982500840523
Correction Precision: 0.8756892230574247, Recall: 0.6527181019987571, F1-Score: 0.7479396334610322
Correction Precision: 0.9083616917839308, Recall: 0.6974617394548903, F1-Score: 0.7890624995084576
Correction Precision: 0.925511661113535, Recall: 0.7261015683344424, F1-Score: 0.813768570337836
Correction Precision: 0.9282690498586715, Recall: 0.73596867424935, F1-Score: 0.8210088398601191
Correction Precision: 0.9313634248485501, Recall: 0.7468283582088159, F1-Score: 0.8289500926928691
Correction Precision: 0.9321836401768976, Recall: 0.7465472191114694, F1-Score: 0.8291014607972677
Correction Precision: 0.9314272431425077, Recall: 0.7478536767449144, F1-Score: 0.8296066247646023
Correction Precision: 0.9320614239178845, Recall: 0.7465523667534948, F1-Score: 0.8290562908966633
Correction Precision: 0.9311787956287069, Recall: 0.7467835166882814, F1-Score: 0.8288493372542108
Epoch 11 Training: 100% 250/250 [03:24<00:00,  1.22it/s, loss=0.0143, c_precision=0.985, c_recall=0.876, c_f1_score=0.927]
Correction Precision: 0.9315227483748998, Recall: 0.7479962721340637, F1-Score: 0.8297322438974042
Eval情况：Eval情况是目前看起来最好的，果然迁移学习的时候原始模型不能训练的太多。
Character-level Detect Acc: 0.9804, P: 0.5347, R: 0.5233, F1: 0.5289
Character-level Correct Acc: 0.9779, P: 0.4721, R: 0.4074, F1: 0.4374
Sentence-level Detect Acc: 0.6082, P: 0.6557, R: 0.4404, F1: 0.5269
Sentence-level Correct Acc: 0.5627, P: 0.6013, R: 0.3486, F1: 0.4413

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss
只训练head，不训练BERT
执行情况：训练了17个epoch就停了， 而且F1只有0.679，说明完全不训练BERT也不行。训练集拟合的也不够好
Correction Precision: 0.22276422764225226, Recall: 0.38346706475081477, F1-Score: 0.28181568796076834
Correction Precision: 0.4530496925377342, Recall: 0.5082028337060946, F1-Score: 0.4790440202378767
Correction Precision: 0.5679611650484376, Recall: 0.5682794694562734, F1-Score: 0.5681202721677433
Correction Precision: 0.6208612440190199, Recall: 0.6054497946994017, F1-Score: 0.6130586785134752
Correction Precision: 0.6635643564355121, Recall: 0.6256534727407345, F1-Score: 0.6440515082452632
Correction Precision: 0.6767755820777388, Recall: 0.644974827521789, F1-Score: 0.6604926479630236
Correction Precision: 0.6950690335304348, Recall: 0.6574626865670414, F1-Score: 0.6757430483976682
Correction Precision: 0.6969397828231595, Recall: 0.6588279208658717, F1-Score: 0.6773481718114999
Correction Precision: 0.6981317600785254, Recall: 0.6625606569614291, F1-Score: 0.6798812596744909
Correction Precision: 0.6969935154252904, Recall: 0.6610137905328622, F1-Score: 0.6785270200645444
Correction Precision: 0.6962919364330593, Recall: 0.6617564795821999, F1-Score: 0.6785850855422585
Correction Precision: 0.6965719882466803, Recall: 0.662814538676484, F1-Score: 0.6792741160235786
Correction Precision: 0.6984470218201889, Recall: 0.6625023307848849, F1-Score: 0.6799999995002185
Correction Precision: 0.6980946768806328, Recall: 0.6623183004098654, F1-Score: 0.6797360614682755
Correction Precision: 0.6968565815322796, Recall: 0.6615069004101712, F1-Score: 0.6787217752369098
Correction Precision: 0.6959340011784136, Recall: 0.6617482256255768, F1-Score: 0.6784107223340788
Correction Precision: 0.697172034563885, Recall: 0.6621898899457821, F1-Score: 0.6792308423204441
Epoch 17 Training: 100% 250/250 [03:23<00:00,  1.23it/s, loss=0.0168, c_precision=0.909, c_recall=0.818, c_f1_score=0.861]
Correction Precision: 0.6976424361491752, Recall: 0.6620059656971174, F1-Score: 0.679357183353284
Eval情况：不仅Wang271K表现查，泛化性也很差。
Character-level Detect Acc: 0.9717, P: 0.3948, R: 0.6478, F1: 0.4906
Character-level Correct Acc: 0.9664, P: 0.2866, R: 0.3989, F1: 0.3335
Sentence-level Detect Acc: 0.4882, P: 0.4806, R: 0.4092, F1: 0.4420
Sentence-level Correct Acc: 0.4200, P: 0.3805, R: 0.2716, F1: 0.3169

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss
训练head，微调BERT。同时对bias稍大学习率和weigh_decay=0的操作
执行情况：训练了17个epoch手动停了，和之前差不多，最高到83.0，但是**这次训练集没有过拟合哎，也许还有成长空间**
Correction Precision: 0.4399181166836356, Recall: 0.40100765068102234, F1-Score: 0.41956267033269495
Correction Precision: 0.7899662074341591, Recall: 0.5665548098432948, F1-Score: 0.6598632065484855
Correction Precision: 0.875532181317086, Recall: 0.6530917242666442, F1-Score: 0.7481275407045209
Correction Precision: 0.9093560145805809, Recall: 0.6983949234787796, F1-Score: 0.7900348353576975
Correction Precision: 0.9253092293052032, Recall: 0.7262882748318285, F1-Score: 0.8138075308878431
Correction Precision: 0.9278326281144974, Recall: 0.73596867424935, F1-Score: 0.8208380987058127
Correction Precision: 0.9315801722129551, Recall: 0.7468283582088159, F1-Score: 0.8290359319900948
Correction Precision: 0.9326183259498874, Recall: 0.7465472191114694, F1-Score: 0.8292733487337067
Correction Precision: 0.9323255813951319, Recall: 0.7482269503544702, F1-Score: 0.8301925859626514
Correction Precision: 0.9329608938545313, Recall: 0.7469250838612099, F1-Score: 0.8296418955937266
Correction Precision: 0.9320772272619371, Recall: 0.7471564422896239, F1-Score: 0.8294348991125984
Correction Precision: 0.9324198792380556, Recall: 0.7483690587137467, F1-Score: 0.8303174434102802
Epoch 12 Training: 100% 250/250 [03:28<00:00,  1.20it/s, loss=0.0171, c_precision=0.988, c_recall=0.884, c_f1_score=0.933]
Correction Precision: 0.9327594229872189, Recall: 0.7475293678909662, F1-Score: 0.8299347888651837
Eval情况：还不错，但是没有不加好。其实差不多
Character-level Detect Acc: 0.9803, P: 0.5324, R: 0.5233, F1: 0.5278
Character-level Correct Acc: 0.9779, P: 0.4707, R: 0.4088, F1: 0.4375
Sentence-level Detect Acc: 0.6064, P: 0.6530, R: 0.4385, F1: 0.5247
Sentence-level Correct Acc: 0.5609, P: 0.5981, R: 0.3468, F1: 0.4390

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。
使用了HeKaiming初始化head，其中参数为mode="fan_in", activation='relu'
执行情况：无法过拟合训练集，验证集上的表现也不够好。也许是我没用对
Correction Precision: 0.2652103559870121, Recall: 0.30715892053967253, F1-Score: 0.28464744653285506
Correction Precision: 0.66623975409819, Recall: 0.487078651685302, F1-Score: 0.5627434006369981
Correction Precision: 0.8107538218237188, Recall: 0.5760299625467086, F1-Score: 0.6735274792601805
Correction Precision: 0.8625857251712312, Recall: 0.6357169599399782, F1-Score: 0.7319754278979853
Correction Precision: 0.888608870967518, Recall: 0.661042369703663, F1-Score: 0.7581165335893122
Correction Precision: 0.8927768057983272, Recall: 0.6685382743775653, F1-Score: 0.7645547940306977
Correction Precision: 0.9002006018051906, Recall: 0.6732933233307064, F1-Score: 0.770386265604653
Correction Precision: 0.9022388059699248, Recall: 0.6787050898202323, F1-Score: 0.7746689444051742
Correction Precision: 0.9022631186269828, Recall: 0.6792735442799701, F1-Score: 0.7750480661621989
Correction Precision: 0.9014435042307364, Recall: 0.6782771535579254, F1-Score: 0.7740970287895736
Correction Precision: 0.9017168449860906, Recall: 0.6786516853931313, F1-Score: 0.774441713368114
Eval情况：一般
Character-level Detect Acc: 0.9763, P: 0.4420, R: 0.4851, F1: 0.4626
Character-level Correct Acc: 0.9735, P: 0.3679, R: 0.3564, F1: 0.3621
Sentence-level Detect Acc: 0.5400, P: 0.5531, R: 0.3725, F1: 0.4452
Sentence-level Correct Acc: 0.4945, P: 0.4826, R: 0.2807, F1: 0.3550

模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。
使用了orthogonal_初始化head，其中参数为gain=1
执行情况：执行了10+epoch手动停了，未完全过拟合训练集，但验证集效果就挺好
Correction Precision: 0.3657561625582132, Recall: 0.3086581709144849, F1-Score: 0.3347901204507666
Correction Precision: 0.7833572453369345, Recall: 0.511235955056084, F1-Score: 0.6186968833746426
Correction Precision: 0.8852281933564987, Recall: 0.6138576779025067, F1-Score: 0.7249806475312637
Correction Precision: 0.909716908951566, Recall: 0.6677274429051538, F1-Score: 0.7701608545261743
Correction Precision: 0.9325416877208357, Recall: 0.6919760029994952, F1-Score: 0.79444683549874
Correction Precision: 0.9406800694959194, Recall: 0.7093393224778758, F1-Score: 0.8087921463299719
Correction Precision: 0.9439252336446277, Recall: 0.7198049512376744, F1-Score: 0.8167695249398422
Correction Precision: 0.9421727558671806, Recall: 0.7286676646705222, F1-Score: 0.8217790435092568
Correction Precision: 0.943286476005588, Recall: 0.7287024901702436, F1-Score: 0.8222245690654718
Correction Precision: 0.9425872093020972, Recall: 0.7286516853931219, F1-Score: 0.821926488734889
Epoch 10 Training: 100% 250/250 [03:30<00:00,  1.19it/s, loss=0.0104, c_precision=0.987, c_recall=0.864, c_f1_score=0.922]
Correction Precision: 0.9429262394193607, Recall: 0.7301498127339456, F1-Score: 0.8230079150751954
Eval情况：比之前的都好，看起来参数初始化方式确实挺能影响模型泛化性。
Character-level Detect Acc: 0.9816, P: 0.5705, R: 0.5205, F1: 0.5444
Character-level Correct Acc: 0.9797, P: 0.5216, R: 0.4272, F1: 0.4697
Sentence-level Detect Acc: 0.6182, P: 0.6812, R: 0.4312, F1: 0.5281
Sentence-level Correct Acc: 0.5827, P: 0.6405, R: 0.3596, F1: 0.4606

模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1
在head前增加了(-0.1, 0.1)的noise
执行情况：执行了12个epoch后手动停了，训练集也没有完全拟合，但感觉学不下去了。可能是noise加的太多了？
Correction Precision: 0.35914074737069607, Recall: 0.30078710644672024, F1-Score: 0.3273839872651889
Correction Precision: 0.7795790671215075, Recall: 0.5132958801497166, F1-Score: 0.6190153563413053
Correction Precision: 0.8831978319780804, Recall: 0.6102996254680505, F1-Score: 0.7218161678443304
Correction Precision: 0.9098867147268512, Recall: 0.6615499812802954, F1-Score: 0.7660958156840096
Correction Precision: 0.9318239633678627, Recall: 0.6867266591674752, F1-Score: 0.7907177545031997
Correction Precision: 0.9362720403020311, Recall: 0.6956765861874048, F1-Score: 0.7982390202344156
Correction Precision: 0.939067201604579, Recall: 0.7023630907725614, F1-Score: 0.8036480681797531
Correction Precision: 0.9397470865358444, Recall: 0.7092065868262145, F1-Score: 0.8083608825212251
Correction Precision: 0.9405351833496183, Recall: 0.710728328028326, F1-Score: 0.8096406095127108
Correction Precision: 0.9396189062111013, Recall: 0.7110486891384435, F1-Score: 0.8095085806841293
Correction Precision: 0.941147378832606, Recall: 0.7127340823968702, F1-Score: 0.8111679449484092
Epoch 11 Training: 100% 250/250 [03:27<00:00,  1.20it/s, loss=0.0211, c_precision=0.989, c_recall=0.826, c_f1_score=0.9]
Correction Precision: 0.940841584158183, Recall: 0.7127320457527259, F1-Score: 0.8110530241545996
Eval情况：Eval情况也没有变好，甚至变差了2个点
Character-level Detect Acc: 0.9813, P: 0.5644, R: 0.5021, F1: 0.5314
Character-level Correct Acc: 0.9794, P: 0.5142, R: 0.4102, F1: 0.4563
Sentence-level Detect Acc: 0.6118, P: 0.6746, R: 0.4183, F1: 0.5164
Sentence-level Correct Acc: 0.5736, P: 0.6284, R: 0.3413, F1: 0.4423

模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1
在head前增加了(-0.05, 0.05)的noise
执行情况：17个epoch后early stop了，同样没有过拟合训练集，且验证集上表现一般
Correction Precision: 0.3627012522360548, Recall: 0.3039730134931964, F1-Score: 0.3307504073341551
Correction Precision: 0.7807790730734203, Recall: 0.5142322097377314, F1-Score: 0.6200745168519668
Correction Precision: 0.8845633955120615, Recall: 0.612734082396889, F1-Score: 0.7239738904334562
Correction Precision: 0.910095042383532, Recall: 0.6632347435416205, F1-Score: 0.7672983211147419
Correction Precision: 0.9323843416367736, Recall: 0.6876640419946217, F1-Score: 0.7915407850097278
Correction Precision: 0.9361809045223779, Recall: 0.6973610331273259, F1-Score: 0.7993135251996711
Correction Precision: 0.9388471177942509, Recall: 0.702550637659283, F1-Score: 0.8036901947472633
Correction Precision: 0.9395590785234235, Recall: 0.7097679640717234, F1-Score: 0.8086557931349462
Correction Precision: 0.9405940594057077, Recall: 0.7114772514509059, F1-Score: 0.810148171346135
Correction Precision: 0.9396935244683787, Recall: 0.7119850187264584, F1-Score: 0.8101427653307668
Correction Precision: 0.9407114624503605, Recall: 0.7131086142320762, F1-Score: 0.8112484017248914
Correction Precision: 0.9402469135800148, Recall: 0.7140446277891028, F1-Score: 0.8116806986459119
Correction Precision: 0.9411619891676659, Recall: 0.7149803628201393, F1-Score: 0.8126262084580319
Correction Precision: 0.9402911423634492, Recall: 0.7135367908630006, F1-Score: 0.811368958419169
Correction Precision: 0.9403500123241458, Recall: 0.7148210605207579, F1-Score: 0.8122205658280371
Correction Precision: 0.941481481481249, Recall: 0.7147141518274198, F1-Score: 0.8125732547036595
Correction Precision: 0.9405793513243524, Recall: 0.7130255255253917, F1-Score: 0.8111455103452085
Epoch 17 Training: 100% 250/250 [03:22<00:00,  1.23it/s, loss=0.025, c_precision=0.988, c_recall=0.826, c_f1_score=0.9]
Correction Precision: 0.9405964998765244, Recall: 0.7146067415728998, F1-Score: 0.8121740976255678
Eval情况：不如无噪音的好
Character-level Detect Acc: 0.9814, P: 0.5660, R: 0.5035, F1: 0.5329
Character-level Correct Acc: 0.9794, P: 0.5151, R: 0.4102, F1: 0.4567
Sentence-level Detect Acc: 0.6127, P: 0.6766, R: 0.4183, F1: 0.5170
Sentence-level Correct Acc: 0.5736, P: 0.6293, R: 0.3394, F1: 0.4410

模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1
在head前增加了(-0.01, 0.01)的noise
执行情况：14个epoch后手动停了了，同样没有过拟合训练集，且验证集上表现一般
Correction Precision: 0.36375838926166354, Recall: 0.30472263868060256, F1-Score: 0.3316336931607767
Correction Precision: 0.7818181818179597, Recall: 0.5153558052433491, F1-Score: 0.62121896114624
Correction Precision: 0.88465694219317, Recall: 0.6132958801496978, F1-Score: 0.7243972567602208
Correction Precision: 0.9099999999997667, Recall: 0.6643579183825038, F1-Score: 0.7680155805550702
Correction Precision: 0.9324358648714929, Recall: 0.6882264716909096, F1-Score: 0.7919318299501854
Correction Precision: 0.9366993217782124, Recall: 0.6979225154406329, F1-Score: 0.7998712993817971
Correction Precision: 0.9391435011267371, Recall: 0.7033008252061697, F1-Score: 0.8042895437460618
Correction Precision: 0.9395890071797625, Recall: 0.7101422155687294, F1-Score: 0.8089097298729225
Correction Precision: 0.940841584158183, Recall: 0.7116644823065509, F1-Score: 0.8103613682334605
Correction Precision: 0.9397233201578706, Recall: 0.7123595505616643, F1-Score: 0.8103962500418921
Correction Precision: 0.9407407407405085, Recall: 0.713483146067282, F1-Score: 0.8115015969533532
Correction Precision: 0.9402321560876907, Recall: 0.7138571160696204, F1-Score: 0.8115540391595863
Correction Precision: 0.940915805021925, Recall: 0.714793342060835, F1-Score: 0.8124136460178151
Epoch 13 Training: 100% 250/250 [03:26<00:00,  1.21it/s, loss=0.018, c_precision=0.989, c_recall=0.821, c_f1_score=0.897]
Correction Precision: 0.9400444115467703, Recall: 0.7133495600073556, F1-Score: 0.811156056566923
Eval情况：不如无噪音的好
Character-level Detect Acc: 0.9813, P: 0.5644, R: 0.5021, F1: 0.5314
Character-level Correct Acc: 0.9794, P: 0.5142, R: 0.4102, F1: 0.4563
Sentence-level Detect Acc: 0.6127, P: 0.6766, R: 0.4183, F1: 0.5170
Sentence-level Correct Acc: 0.5736, P: 0.6293, R: 0.3394, F1: 0.4410


模型：模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1
为模型增加了一个特征，832+1，这一个特征是该字是否有错，然后看看在告诉模型这个字是错的前提下，模型的表现如何。
执行情况：比之前好了1.5个点，但并没有好处太多，看来模型不太会利用这个特征。也可以理解，800多个特征，他根本就不知道最后一个特征居然这么重要。
Correction Precision: 0.3687031082528684, Recall: 0.3158281307380984, F1-Score: 0.34022351844268445
Correction Precision: 0.7748719331354071, Recall: 0.5275330396474802, F1-Score: 0.6277165005553533
Correction Precision: 0.877087682672005, Recall: 0.6170369010463342, F1-Score: 0.7244315115313713
Correction Precision: 0.9122287968439565, Recall: 0.6790236740685118, F1-Score: 0.7785376112938471
Correction Precision: 0.9326695116166588, Recall: 0.7220998531569893, F1-Score: 0.8139871710369987
Correction Precision: 0.9389044943818026, Recall: 0.736368643289749, F1-Score: 0.8253935585152664
Correction Precision: 0.9417587476977574, Recall: 0.7506422018347246, F1-Score: 0.8354094338537651
Correction Precision: 0.9418257070588774, Recall: 0.7519735634292726, F1-Score: 0.8362596973419621
Correction Precision: 0.9427786678871726, Recall: 0.7560572687223281, F1-Score: 0.8391565646479696
Correction Precision: 0.9428832533696725, Recall: 0.7578038927651931, F1-Score: 0.8402728285803711
Correction Precision: 0.9421978524101113, Recall: 0.7569750367105805, F1-Score: 0.839491093653353
Eval情况：Eval情况也比之前好了很多，但很明显没有把最后一个特征体现出来。
Character-level Detect Acc: 0.9845, P: 0.6667, R: 0.5318, F1: 0.5917
Character-level Correct Acc: 0.9823, P: 0.6163, R: 0.4272, F1: 0.5046
Sentence-level Detect Acc: 0.6564, P: 0.7393, R: 0.4734, F1: 0.5772
Sentence-level Correct Acc: 0.6136, P: 0.6987, R: 0.3872, F1: 0.4982

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1
对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。
执行情况：执行了11个epoch手动停了，看起来效果不错。
Correction Precision: 0.9538606403004201, Recall: 0.1888868170799573, F1-Score: 0.31533073902357184
Correction Precision: 0.9680511182105189, Recall: 0.5084840574304478, F1-Score: 0.6667481658074429
Correction Precision: 0.9744047619044719, Recall: 0.6100242220978926, F1-Score: 0.7503151135402522
Correction Precision: 0.9756367663341706, Recall: 0.656238361266172, F1-Score: 0.7846804715742007
Correction Precision: 0.9694323144102314, Recall: 0.7034482758619378, F1-Score: 0.8152948796161581
Correction Precision: 0.9730964467002606, Recall: 0.7152985074625531, F1-Score: 0.8245161285437375
Correction Precision: 0.9738285144563874, Recall: 0.727696032780643, F1-Score: 0.8329602382907986
Correction Precision: 0.9733698357389314, Recall: 0.7288483041370241, F1-Score: 0.8335464615732259
Correction Precision: 0.973684210526074, Recall: 0.7310344827584844, F1-Score: 0.8350899601189166
Correction Precision: 0.9732342007432532, Recall: 0.731557377049044, F1-Score: 0.835265340358536
Epoch 10 Training: 100% 250/250 [03:38<00:00,  1.14it/s, loss=0.0184, c_precision=0.99, c_recall=0.871, c_f1_score=0.927]
Correction Precision: 0.9725519287831423, Recall: 0.733768656716281, F1-Score: 0.8364525728826465
Eval情况：Eval情况相当不错，尤其是精准率提高了超多。没想到这么一个简单的改造可以提升这么多。
Character-level Detect Acc: 0.9865, P: 0.7576, R: 0.5304, F1: 0.6240
Character-level Correct Acc: 0.9842, P: 0.7115, R: 0.4187, F1: 0.5272
Sentence-level Detect Acc: 0.6882, P: 0.7971, R: 0.4972, F1: 0.6124
Sentence-level Correct Acc: 0.6336, P: 0.7536, R: 0.3872, F1: 0.5115

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。
对FocalLoss的alpha参数做了调整，index 1的alpha为0.25，其余为0.75
执行情况：执行了9个epoch后手动停了。效果比使用默认alpha效果好一点（可忽略不计），可能alpha参数是锦上添花？
Correction Precision: 0.8901560624244357, Recall: 0.27652433339543603, F1-Score: 0.42196613991419835
Correction Precision: 0.9221222463540422, Recall: 0.5541674435948994, F1-Score: 0.69228977358168
Correction Precision: 0.9445496348388032, Recall: 0.6506428172162007, F1-Score: 0.7705207409113309
Correction Precision: 0.9514811031662535, Recall: 0.6938547486032227, F1-Score: 0.802498384177173
Correction Precision: 0.945894182427353, Recall: 0.7364398881638887, F1-Score: 0.8281282745023366
Correction Precision: 0.9450860240393718, Recall: 0.7481343283580694, F1-Score: 0.8351556800230961
Correction Precision: 0.9535104364324113, Recall: 0.7487427826409482, F1-Score: 0.8388106411346029
Correction Precision: 0.9518981372315604, Recall: 0.75232948192308, F1-Score: 0.840428853467335
Correction Precision: 0.9537517697024648, Recall: 0.7534016775394681, F1-Score: 0.8418202640073881
Epoch 9 Training: 100% 250/250 [03:37<00:00,  1.15it/s, loss=0.00484, c_precision=0.964, c_recall=0.911, c_f1_score=0.937]
Correction Precision: 0.9540094339620391, Recall: 0.7535394932934513, F1-Score: 0.8420066606224532
Eval情况：如果比较Character-level的话，没有变差。
Character-level Detect Acc: 0.9856, P: 0.6777, R: 0.6068, F1: 0.6403
Character-level Correct Acc: 0.9825, P: 0.6129, R: 0.4569, F1: 0.5235
Sentence-level Detect Acc: 0.6727, P: 0.7342, R: 0.5321, F1: 0.6170
Sentence-level Correct Acc: 0.6045, P: 0.6719, R: 0.3945, F1: 0.4971

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。
bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings
执行情况：执行了14个epoch后手动停了（前面的数据丢了）。这是首次模型在Wang271K上到达0.86。很有前景
Correction Precision: 0.9673913043476022, Recall: 0.7799738952077605, F1-Score: 0.8636316708176413
Correction Precision: 0.967794253938608, Recall: 0.7788551184037331, F1-Score: 0.8631056922425251
Correction Precision: 0.9683017121700674, Recall: 0.779765231973024, F1-Score: 0.8638662395716922
Correction Precision: 0.9678761266464136, Recall: 0.7798882681562793, F1-Score: 0.8637723002171666
Epoch 13 Training: 100% 250/250 [03:40<00:00,  1.13it/s, loss=0.00707, c_precision=0.989, c_recall=0.923, c_f1_score=0.955]
Correction Precision: 0.9680555555553314, Recall: 0.7794967381172824, F1-Score: 0.863603510089019
Eval情况：Eval情况只能用没降来形容。
Character-level Detect Acc: 0.9835, P: 0.6092, R: 0.5997, F1: 0.6044
Character-level Correct Acc: 0.9815, P: 0.5676, R: 0.5050, F1: 0.5344
Sentence-level Detect Acc: 0.6318, P: 0.6691, R: 0.5083, F1: 0.5777
Sentence-level Correct Acc: 0.5891, P: 0.6267, R: 0.4220, F1: 0.5044

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings
对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
代码：MultiModalBert1.py
执行情况：执行了10个epoch手动停了，在Wang271K上看起来差不多
Correction Precision: 0.9642267473852701, Recall: 0.32414431082325174, F1-Score: 0.48518415913587865
Correction Precision: 0.9578787878784976, Recall: 0.5849370836416385, F1-Score: 0.7263327201172844
Correction Precision: 0.965072541644018, Recall: 0.6653083904425513, F1-Score: 0.7876329345019286
Correction Precision: 0.9607265586644672, Recall: 0.723207686622187, F1-Score: 0.8252161074583292
Correction Precision: 0.9585492227977016, Recall: 0.7525887573963105, F1-Score: 0.8431738134701932
Correction Precision: 0.9552476147203645, Recall: 0.7779833487510124, F1-Score: 0.8575507285761267
Correction Precision: 0.9677938808371364, Recall: 0.777346637102591, F1-Score: 0.862178501401464
Correction Precision: 0.9663846329748532, Recall: 0.7808573540279414, F1-Score: 0.8637710776863499
Correction Precision: 0.9678825418671786, Recall: 0.7794199150192537, F1-Score: 0.8634875148556195
Epoch 9 Training: 100% 250/250 [03:37<00:00,  1.15it/s, loss=0.00223, c_precision=0.99, c_recall=0.939, c_f1_score=0.964]
Correction Precision: 0.9729417206288221, Recall: 0.7760560782142084, F1-Score: 0.8634171364994887
Eval情况：Eval上的效果还是挺明显的，说明遗忘门有效。达到了我的sota
Character-level Detect Acc: 0.9843, P: 0.6421, R: 0.5785, F1: 0.6086
Character-level Correct Acc: 0.9827, P: 0.6089, R: 0.5021, F1: 0.5504
Sentence-level Detect Acc: 0.6445, P: 0.6944, R: 0.5046, F1: 0.5845
Sentence-level Correct Acc: 0.6109, P: 0.6630, R: 0.4367, F1: 0.5265
一次全量训练情况：Wang271K上全量好像效果并不是很好
Epoch 0 Training: 100% 6783/6783 [1:40:33<00:00,  1.12it/s, loss=0.0398, c_precision=0.959, c_recall=0.64, c_f1_score=0.767]
Correction Precision: 0.9628663109873781, Recall: 0.6539966388319854, F1-Score: 0.778929754944518
一次全量训练的Eval情况：不过Eval情况也一般，距离PyCorrector还很远
Character-level Detect Acc: 0.9865, P: 0.7490, R: 0.5403, F1: 0.6278
Character-level Correct Acc: 0.9853, P: 0.7277, R: 0.4837, F1: 0.5811
Sentence-level Detect Acc: 0.6945, P: 0.8255, R: 0.4862, F1: 0.6120
Sentence-level Correct Acc: 0.6745, P: 0.8127, R: 0.4459, F1: 0.5758



模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门
遗忘门权重使用了concat(token_embedding, bert_hidden_states) 共同来计算。
执行情况：13个epoch后early stop了。在Wang271K上的表现不如只使用token_embedding的遗忘门。
Correction Precision: 0.9682179341651713, Recall: 0.3161601186063165, F1-Score: 0.4766694603719625
Correction Precision: 0.9615728328862194, Recall: 0.5974458634091064, F1-Score: 0.7369863008969701
Correction Precision: 0.9680738786277129, Recall: 0.6789415247963214, F1-Score: 0.7981292142204777
Correction Precision: 0.9599027946534726, Recall: 0.731481481481346, F1-Score: 0.8302679974070003
Correction Precision: 0.9674816045570929, Recall: 0.7545353572749436, F1-Score: 0.8478419131840166
Correction Precision: 0.9696466682473394, Recall: 0.7570820218476657, F1-Score: 0.8502807231505692
Correction Precision: 0.9675446848539586, Recall: 0.7614288358318042, F1-Score: 0.8522009316664321
Correction Precision: 0.9674396814239944, Recall: 0.7646732086649204, F1-Score: 0.8541882104684123
Correction Precision: 0.9675294117644782, Recall: 0.7623285131626321, F1-Score: 0.8527581911285925
Correction Precision: 0.9678780773737472, Recall: 0.7631724902938134, F1-Score: 0.8534215417850872
Correction Precision: 0.9678177120035311, Recall: 0.7626804887077447, F1-Score: 0.8530903815339743
Correction Precision: 0.9679999999997722, Recall: 0.7622753381506833, F1-Score: 0.8529076391876211
Epoch 12 Training: 100% 250/250 [03:40<00:00,  1.13it/s, loss=0.0172, c_precision=0.986, c_recall=0.858, c_f1_score=0.917]
Correction Precision: 0.9676056338025897, Recall: 0.7631920014810658, F1-Score: 0.8533278123626096
Eval情况：不如只使用token_embedding的遗忘门
Character-level Detect Acc: 0.9835, P: 0.6240, R: 0.5446, F1: 0.5816
Character-level Correct Acc: 0.9818, P: 0.5850, R: 0.4625, F1: 0.5166
Sentence-level Detect Acc: 0.6264, P: 0.6727, R: 0.4789, F1: 0.5595
Sentence-level Correct Acc: 0.5891, P: 0.6340, R: 0.4037, F1: 0.4933


模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门
给bert_outputs.last_hidden_state也加了遗忘门
执行情况：执行了11个epoch后手动停了。在Wang271K上增加遗忘门后performance降低了，但不是很明显
Correction Precision: 0.9463120923226561, Recall: 0.35029717682014294, F1-Score: 0.5113189639541617
Correction Precision: 0.9425634824664624, Recall: 0.5795539033456172, F1-Score: 0.7177716385706308
Correction Precision: 0.9336747759280579, Recall: 0.6773174809584475, F1-Score: 0.7850990520534051
Correction Precision: 0.9529935675405856, Recall: 0.716251394570339, F1-Score: 0.8178343944143421
Correction Precision: 0.938592456301532, Recall: 0.7572833549822309, F1-Score: 0.8382458657887922
Correction Precision: 0.9555763385548385, Recall: 0.7581153774808461, F1-Score: 0.8454695899077874
Correction Precision: 0.957195742711486, Recall: 0.7685305591676075, F1-Score: 0.8525502313450583
Correction Precision: 0.9556684368503912, Recall: 0.768331167625623, F1-Score: 0.8518213619259796
Correction Precision: 0.9563611175246002, Recall: 0.7694594092512038, F1-Score: 0.8527897874406277
Correction Precision: 0.9563106796114293, Recall: 0.7686733556297345, F1-Score: 0.8522867733005488
Epoch 10 Training: 100% 250/250 [03:40<00:00,  1.14it/s, loss=0.0071, c_precision=0.98, c_recall=0.89, c_f1_score=0.933]
Correction Precision: 0.956351039260749, Recall: 0.7687024317800688, F1-Score: 0.8523206746112261
Eval情况：这个降的比较明显
Character-level Detect Acc: 0.9827, P: 0.5831, R: 0.6252, F1: 0.6034
Character-level Correct Acc: 0.9801, P: 0.5298, R: 0.5035, F1: 0.5163
Sentence-level Detect Acc: 0.6136, P: 0.6364, R: 0.5138, F1: 0.5685
Sentence-level Correct Acc: 0.5618, P: 0.5822, R: 0.4092, F1: 0.4806

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
将PlateauScheduler改为WarmupExponentialLR，学习率不变，head为2e-4，bert为2e-6
执行情况：执行了16个epoch手动停了，感觉涨不了多少了。在Wang271K上比PlateauScheduler好了0.05个点。
Correction Precision: 0.0, Recall: 0.0, F1-Score: 0.0
Correction Precision: 0.9961538461500148, Recall: 0.0479363316675832, F1-Score: 0.09147095170469562
Correction Precision: 0.9654561558897408, Recall: 0.40340488527009555, F1-Score: 0.569042025164935
Correction Precision: 0.9568323905921534, Recall: 0.5951851851850749, F1-Score: 0.7338737294048209
Correction Precision: 0.9626806833111793, Recall: 0.6780821917806963, F1-Score: 0.7956989242460532
Correction Precision: 0.9718488460560558, Recall: 0.7094982410663377, F1-Score: 0.8202054789640528
Correction Precision: 0.9702539298667544, Recall: 0.7425504349434124, F1-Score: 0.8412665124044456
Correction Precision: 0.9647031323045897, Recall: 0.7641177559709749, F1-Score: 0.8527740464122266
Correction Precision: 0.9611988847581409, Recall: 0.7669632925471325, F1-Score: 0.8531656006610174
Correction Precision: 0.9645977011492034, Recall: 0.7757441301533045, F1-Score: 0.8599241720643751
Correction Precision: 0.9683090446446985, Recall: 0.7748981858569465, F1-Score: 0.8608740354956952
Correction Precision: 0.9566092596732371, Recall: 0.7924773022047817, F1-Score: 0.8668423181098207
Correction Precision: 0.9649083088066639, Recall: 0.7891131271985208, F1-Score: 0.868201262491197
Correction Precision: 0.9650555933739584, Recall: 0.7871552841012793, F1-Score: 0.867074413368382
Correction Precision: 0.9726218097445538, Recall: 0.7764400814964295, F1-Score: 0.8635286842316247
Epoch 15 Training: 100% 250/250 [03:42<00:00,  1.12it/s, c_precision=0.991, c_recall=0.975, c_f1_score=0.983, loss=0.0012]
Correction Precision: 0.9726938962824752, Recall: 0.7838387573963047, F1-Score: 0.8681138639332079
Eval情况：Eval情况相比PlateauScheduler差不多。可以得出结论，WarmupExponentialLR和PlateauScheduler在小数据量下好像区别不大。
Character-level Detect Acc: 0.9836, P: 0.6158, R: 0.5827, F1: 0.5988
Character-level Correct Acc: 0.9819, P: 0.5808, R: 0.5035, F1: 0.5394
Sentence-level Detect Acc: 0.6355, P: 0.6782, R: 0.5028, F1: 0.5774
Sentence-level Correct Acc: 0.6018, P: 0.6458, R: 0.4349, F1: 0.5197

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
将PlateauScheduler改为WarmupExponentialLR，学习率不变，head为2e-4，bert为2e-6
全量训练：
Epoch 0 Training: 100% 8055/8055 [1:57:18<00:00,  1.14it/s, c_precision=0.957, c_recall=0.784, c_f1_score=0.862, loss=0.0157]
Correction Precision: 0.9588022005500776, Recall: 0.8061074319352041, F1-Score: 0.8758494655536039
Epoch 1 Training: 100% 8055/8055 [1:57:13<00:00,  1.15it/s, c_precision=0.964, c_recall=0.843, c_f1_score=0.9, loss=0.0115]
Correction Precision: 0.971353690497899, Recall: 0.8379515221620044, F1-Score: 0.8997346576633403
Epoch 2 Training: 100% 8055/8055 [1:58:16<00:00,  1.14it/s, c_precision=0.974, c_recall=0.876, c_f1_score=0.922, loss=0.00844]
Correction Precision: 0.9711907043602677, Recall: 0.8522548092084068, F1-Score: 0.9078439052184416
Epoch 3 Training: 100% 8055/8055 [1:57:49<00:00,  1.14it/s, c_precision=0.969, c_recall=0.883, c_f1_score=0.924, loss=0.0101]
Correction Precision: 0.9699346405228182, Recall: 0.8582995951416552, F1-Score: 0.9107088058841393
2个全量Epoch的Eval情况：有点出乎意料
Character-level Detect Acc: 0.9908, P: 0.8031, R: 0.7440, F1: 0.7724
Character-level Correct Acc: 0.9901, P: 0.7965, R: 0.7143, F1: 0.7532
Sentence-level Detect Acc: 0.7764, P: 0.8453, R: 0.6716, F1: 0.7485
Sentence-level Correct Acc: 0.7645, P: 0.8405, R: 0.6477, F1: 0.7316
4个全量epoch的Eval情况：4个Epoch后，结果并没有变得更好，反而变差了。从Char-level上看，还不算变差。
Character-level Detect Acc: 0.9904, P: 0.7673, R: 0.7836, F1: 0.7754
Character-level Correct Acc: 0.9898, P: 0.7600, R: 0.7525, F1: 0.7562
Sentence-level Detect Acc: 0.7627, P: 0.8114, R: 0.6789, F1: 0.7393
Sentence-level Correct Acc: 0.7509, P: 0.8059, R: 0.6550, F1: 0.7227
FineTune方式：使用SIGHAN训练集进行finetune，SGD优化器(lr=0.001, momentum=0.9, weight_decay=0.001)，无scheduler
finetue结果：最好结果为78.2，其他基本都保持在77.5左右。
Character-level Detect Acc: 0.9926, P: 0.8791, R: 0.7511, F1: 0.8101
Character-level Correct Acc: 0.9922, P: 0.8765, R: 0.7327, F1: 0.7982
Sentence-level Detect Acc: 0.8164, P: 0.9074, R: 0.7009, F1: 0.7909
Sentence-level Correct Acc: 0.8100, P: 0.9058, R: 0.6881, F1: 0.7821




模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
去掉了MultiModalBert，仅使用MacBert（并没有使用MacBertHead的预训练参数）
执行情况：执行了16个epoch停了，比MultiModalBert稍微差了一点，但不多
Correction Precision: 0.989473684209832, Recall: 0.2646893185657472, F1-Score: 0.4176540281028811
Correction Precision: 0.9650521152664117, Recall: 0.5912847483094306, F1-Score: 0.7332867453940294
Correction Precision: 0.9682368775232926, Recall: 0.6777840587901492, F1-Score: 0.7973841715393332
Correction Precision: 0.975891254167485, Recall: 0.7141516516515175, F1-Score: 0.8247534405002174
Correction Precision: 0.9669162038152699, Recall: 0.7526315789472269, F1-Score: 0.8464221536139168
Correction Precision: 0.971725471241911, Recall: 0.7558270676690308, F1-Score: 0.8502854721234043
Correction Precision: 0.9688836104510762, Recall: 0.7655780780779343, F1-Score: 0.855315579292809
Correction Precision: 0.9717973231355229, Recall: 0.7629949333832308, F1-Score: 0.8548302318207397
Correction Precision: 0.9709246901808933, Recall: 0.7642093415868009, F1-Score: 0.8552534895877398
Correction Precision: 0.970405727923396, Recall: 0.7638549690022987, F1-Score: 0.8548302318205887
Correction Precision: 0.9703986631652015, Recall: 0.7642413987590215, F1-Score: 0.8550694147361629
Correction Precision: 0.9706723891270932, Recall: 0.7647942889346675, F1-Score: 0.8555216975209367
Correction Precision: 0.9711492608486, Recall: 0.7647390161470587, F1-Score: 0.8556722684144527
Correction Precision: 0.971544715446922, Recall: 0.7640090259494614, F1-Score: 0.855368420559601
Correction Precision: 0.9706583969463333, Recall: 0.7639879834771377, F1-Score: 0.855011556556718
Correction Precision: 0.9704127893101955, Recall: 0.7634691195793573, F1-Score: 0.8545913002003659
Eval情况：同样比MultiModalBert稍微差了一点，但不多
Character-level Detect Acc: 0.9843, P: 0.6552, R: 0.5403, F1: 0.5922
Character-level Correct Acc: 0.9827, P: 0.6193, R: 0.4625, F1: 0.5296
Sentence-level Detect Acc: 0.6445, P: 0.7115, R: 0.4752, F1: 0.5699
Sentence-level Correct Acc: 0.6127, P: 0.6809, R: 0.4110, F1: 0.5126

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
去掉了MultiModalBert，仅使用MacBert（并没有使用MacBertHead的预训练参数），增加了对比学习（错字和对应正确字的编码越近越好，与其他字越远越好）
执行情况：对比学习的loss有在降，但是整体上不去，反而降了。
Epoch 0 Training: 100% 250/250 [04:06<00:00,  1.01it/s, c_precision=0.955, c_recall=0.0673, c_f1_score=0.126, loss=2.15, loss_c=3.97]
Correction Precision: 0.9691358024676402, Recall: 0.11788999436829023, F1-Score: 0.21020920482746994
Correction Precision: 0.9212570621465673, Recall: 0.490045078887962, F1-Score: 0.6397743987618225
Correction Precision: 0.9085586841369087, Recall: 0.6140945920481224, F1-Score: 0.7328536086932593
Correction Precision: 0.9128856624317052, Recall: 0.6608483483482243, F1-Score: 0.7666848117064516
Correction Precision: 0.9252407501264761, Recall: 0.6862781954885927, F1-Score: 0.788042304712637
Correction Precision: 0.9231923942954908, Recall: 0.6936090225562606, F1-Score: 0.7921004610318607
Correction Precision: 0.926433915211739, Recall: 0.6972597597596288, F1-Score: 0.7956735912853311
Correction Precision: 0.9235732009923266, Recall: 0.6984424845185403, F1-Score: 0.7953841217447438
Correction Precision: 0.923611111110882, Recall: 0.6985556180827802, F1-Score: 0.7954715364100201
Epoch 9 Training: 100% 250/250 [04:09<00:00,  1.00it/s, c_precision=0.956, c_recall=0.701, c_f1_score=0.809, loss=0.99, loss_c=1.92]
Correction Precision: 0.9231534444165822, Recall: 0.6973511177905884, F1-Score: 0.7945205474547433
Eval情况：更差了，而且差的不是一星半点
Character-level Detect Acc: 0.9806, P: 0.5431, R: 0.5078, F1: 0.5249
Character-level Correct Acc: 0.9775, P: 0.4559, R: 0.3579, F1: 0.4010
Sentence-level Detect Acc: 0.5700, P: 0.6000, R: 0.3963, F1: 0.4773
Sentence-level Correct Acc: 0.5145, P: 0.5184, R: 0.2844, F1: 0.3673

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
对比学习使用Bert的hidden_state(不融合word_embeddings)
执行情况：执行了8个epoch停了，感觉上不去了，虽然对比学习的loss下降了， 但好像没借用
Epoch 0 Training: 100% 250/250 [04:35<00:00,  1.10s/it, c_precision=0.926, c_recall=0.0596, c_f1_score=0.112, loss=2.04, loss_c=3.75]
Correction Precision: 0.9611197511649128, Recall: 0.11601276515860408, F1-Score: 0.20703517568711802
Correction Precision: 0.8994063324535292, Recall: 0.5122088655145544, F1-Score: 0.6527046429074957
Correction Precision: 0.8837021046948506, Recall: 0.617109478047745, F1-Score: 0.7267280589852702
Correction Precision: 0.89383474034257, Recall: 0.6557807807806576, F1-Score: 0.7565226800356155
Correction Precision: 0.8969149736641844, Recall: 0.6721804511276931, F1-Score: 0.7684538514494919
Correction Precision: 0.8991194968551197, Recall: 0.6718045112780692, F1-Score: 0.7690155992951343
Correction Precision: 0.9002756201451014, Recall: 0.6743618618617352, F1-Score: 0.7711127798513646
Epoch 7 Training: 100% 250/250 [04:33<00:00,  1.09s/it, c_precision=0.946, c_recall=0.649, c_f1_score=0.77, loss=0.688, loss_c=1.28]
Correction Precision: 0.89969909729165, Recall: 0.6732970538561318, F1-Score: 0.7702050011201529
Eval情况：很差
Character-level Detect Acc: 0.9797, P: 0.5185, R: 0.4767, F1: 0.4967
Character-level Correct Acc: 0.9762, P: 0.4139, R: 0.3126, F1: 0.3562
Sentence-level Detect Acc: 0.5600, P: 0.5916, R: 0.3615, F1: 0.4487
Sentence-level Correct Acc: 0.4927, P: 0.4749, R: 0.2257, F1: 0.3060


模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
增加了分词预测，loss占0.7，分词loss占0.3
执行情况：从结果上，模型确实学会了如何分词，但整体性能下降了
Correction Precision: 0.955414012738245, Recall: 0.27639579878380754, F1-Score: 0.428755180443576
Correction Precision: 0.9437201907787143, Recall: 0.5461906514537088, F1-Score: 0.6919221349824555
Correction Precision: 0.9466666666664038, Recall: 0.6277399152697314, F1-Score: 0.7549008744788172
Correction Precision: 0.9455394190868918, Recall: 0.6704670834864526, F1-Score: 0.784592209542293
Correction Precision: 0.9293119698395548, Recall: 0.7262014361994611, F1-Score: 0.8152971571300964
Correction Precision: 0.938713450292178, Recall: 0.7379551305625711, F1-Score: 0.8263152471132991
Correction Precision: 0.9523475823403521, Recall: 0.7501379944800827, F1-Score: 0.839234252285802
Correction Precision: 0.955992509363072, Recall: 0.749357798165, F1-Score: 0.8401563459379434
Correction Precision: 0.9483310470962623, Recall: 0.7627804339829417, F1-Score: 0.845495311368732
Correction Precision: 0.94997694790204, Recall: 0.7586524300440429, F1-Score: 0.843602865422166
Correction Precision: 0.9496667432771891, Recall: 0.7596984739840486, F1-Score: 0.8441266593629998
Correction Precision: 0.9502533394746774, Recall: 0.7585953300237619, F1-Score: 0.8436765151998087
Correction Precision: 0.9503448275859884, Recall: 0.7599264705880956, F1-Score: 0.8445352395468835
Epoch 13 Training: 100% 250/250 [07:13<00:00,  1.73s/it, c_precision=0.977, c_recall=0.865, c_f1_score=0.918, loss=0.0461, loss_ws=0.13, ws_acc=0.963]
Correction Precision: 0.9497927222475933, Recall: 0.7590649733111063, F1-Score: 0.8437851657464651
Eval情况：同样有所下降，我猜测可能是分词loss的权重过高
Character-level Detect Acc: 0.9825, P: 0.5915, R: 0.5488, F1: 0.5693
Character-level Correct Acc: 0.9807, P: 0.5496, R: 0.4625, F1: 0.5023
Sentence-level Detect Acc: 0.6009, P: 0.6380, R: 0.4495, F1: 0.5274
Sentence-level Correct Acc: 0.5700, P: 0.6029, R: 0.3872, F1: 0.4715

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
增加了分词预测，loss占0.7+，分词loss占0.3-，分词loss权重为min(0.3, 1-acc)
代码：MultiModalBertWS.py
执行情况：执行了11个epoch后手动停了。比使用分词loss0.3要好一点。即使给分词loss分配较少的权重，其一样可以学会如何分词。
Correction Precision: 0.9572933998885428, Recall: 0.31803943246723454, F1-Score: 0.4774550480348963
Correction Precision: 0.9403697078112879, Recall: 0.5804195804194736, F1-Score: 0.7177969954313516
Correction Precision: 0.9513297872337896, Recall: 0.6588690366548795, F1-Score: 0.7785395576836992
Correction Precision: 0.9512134100573052, Recall: 0.6991541007722142, F1-Score: 0.8059353466233099
Correction Precision: 0.9407114624503741, Recall: 0.7449825078253093, F1-Score: 0.8314837644058712
Correction Precision: 0.9410430839000132, Recall: 0.7631482162558361, F1-Score: 0.8428107224947164
Correction Precision: 0.9574712643675959, Recall: 0.7663293468259859, F1-Score: 0.8513030143245733
Correction Precision: 0.9624250806821202, Recall: 0.766055045871419, F1-Score: 0.8530854102132672
Correction Precision: 0.959798994974655, Recall: 0.7727105553510899, F1-Score: 0.85615321873956
Correction Precision: 0.9601008942900802, Recall: 0.7708026509571482, F1-Score: 0.8551005815542073
Epoch 10 Training: 100% 250/250 [09:33<00:00,  2.29s/it, c_precision=0.979, c_recall=0.909, c_f1_score=0.943, loss=0.0172, loss_ws=0.196, ws_acc=0.94]
Correction Precision: 0.9591044094126207, Recall: 0.7718330575472013, F1-Score: 0.8553382228145618
Eval情况：还可以，不过没有不加Eval效果好。
Character-level Detect Acc: 0.9833, P: 0.6178, R: 0.5488, F1: 0.5813
Character-level Correct Acc: 0.9817, P: 0.5812, R: 0.4710, F1: 0.5203
Sentence-level Detect Acc: 0.6264, P: 0.6763, R: 0.4716, F1: 0.5557
Sentence-level Correct Acc: 0.5936, P: 0.6424, R: 0.4055, F1: 0.4972



模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
只让模型预测4500个字。
执行情况：执行了13个epoch就early stop了。在Wang271K上比预测所有token要差那么一点点，我觉得这应该是随机波动的原因。
Correction Precision: 0.9726027397255976, Recall: 0.4107109535360308, F1-Score: 0.5775387033346406
Correction Precision: 0.9660818713447468, Recall: 0.6159582401191991, F1-Score: 0.7522768665552835
Correction Precision: 0.9598971722362571, Recall: 0.6963819470345586, F1-Score: 0.8071768261445615
Correction Precision: 0.9644259077524621, Recall: 0.7331219694142608, F1-Score: 0.8330154688882121
Correction Precision: 0.9681159420287515, Recall: 0.7466467958269846, F1-Score: 0.8430795114980663
Correction Precision: 0.9751807228913312, Recall: 0.7547556881759129, F1-Score: 0.8509251466904448
Correction Precision: 0.9737470167062114, Recall: 0.7613360701622016, F1-Score: 0.8545397418887156
Correction Precision: 0.9688372093021002, Recall: 0.775646993111008, F1-Score: 0.8615448242396938
Correction Precision: 0.9747403210573713, Recall: 0.7703358208953786, F1-Score: 0.8605669023828788
Correction Precision: 0.9746685606058297, Recall: 0.7683837252704799, F1-Score: 0.8593195570104761
Correction Precision: 0.9747224190878868, Recall: 0.7700634565134807, F1-Score: 0.8603899484171367
Correction Precision: 0.974486180013944, Recall: 0.7687290346625477, F1-Score: 0.8594645270617404
Epoch 12 Training: 100% 250/250 [03:39<00:00,  1.14it/s, c_precision=0.992, c_recall=0.945, c_f1_score=0.968, loss=0.00336]
Correction Precision: 0.9742255852445082, Recall: 0.7688001492814389, F1-Score: 0.8594075923311295
Eval情况：从Char-level上看，变差了一点，但在Sent-level上看，好了一点。结论：有效，但不多。 不过这可以极大的减少参数量。
Character-level Detect Acc: 0.9844, P: 0.6602, R: 0.5332, F1: 0.5900
Character-level Correct Acc: 0.9831, P: 0.6333, R: 0.4738, F1: 0.5421
Sentence-level Detect Acc: 0.6509, P: 0.7182, R: 0.4862, F1: 0.5799
Sentence-level Correct Acc: 0.6227, P: 0.6923, R: 0.4294, F1: 0.5300

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
只让模型预测6000个字。
执行情况：执行了9个epoch手动停了，感觉上不去了。
Correction Precision: 0.9632690541777028, Recall: 0.3909802459932182, F1-Score: 0.5562036051034585
Correction Precision: 0.9646282973618212, Recall: 0.5994783904618853, F1-Score: 0.7394301465859043
Correction Precision: 0.9539524174978373, Recall: 0.6945427453900735, F1-Score: 0.8038370333557087
Correction Precision: 0.9643124532066473, Recall: 0.7200894521057174, F1-Score: 0.8244958919673926
Correction Precision: 0.9599904739221338, Recall: 0.750791581299916, F1-Score: 0.8426003339554609
Correction Precision: 0.9676352468696036, Recall: 0.7628981188301801, F1-Score: 0.8531555920916988
Correction Precision: 0.9620312136033164, Recall: 0.7692307692306259, F1-Score: 0.8548954662830887
Correction Precision: 0.9652599673581335, Recall: 0.7710933134660511, F1-Score: 0.8573203556874788
Epoch 8 Training: 100% 250/250 [03:45<00:00,  1.11it/s, c_precision=0.99, c_recall=0.944, c_f1_score=0.967, loss=0.00406]
Correction Precision: 0.9741092636577258, Recall: 0.7641140301843181, F1-Score: 0.8564268555163072
Eval情况：首次突破53大关，又一次达到了我的sota
Character-level Detect Acc: 0.9846, P: 0.6616, R: 0.5502, F1: 0.6008
Character-level Correct Acc: 0.9834, P: 0.6369, R: 0.4936, F1: 0.5562
Sentence-level Detect Acc: 0.6409, P: 0.6995, R: 0.4826, F1: 0.5711
Sentence-level Correct Acc: 0.6200, P: 0.6799, R: 0.4404, F1: 0.5345

模型：MultiModalBert，使用macbert作为backbone，macbert的head。加入了FocalLoss中的难易样本的权重。在此基础上，0.01的loss和0.99的soft_loss。训练head，微调BERT。使用了orthogonal_初始化head，其中参数为gain=1。对输出进行了改造，如果该字没错，则固定输出的softmax的index为1，否则softmax就正常输出所在的index。bert的输出融合了一开始的embedding数据，即last_hidden_state += token_embeddings。对token_embedding增加了遗忘门后再加，遗忘门权重只使用了token_emebdding来计算。
只让模型预测6000个字。使用WarmupScheduler
执行情况：执行了18个epoch后手动停了。Warmup确实可以稳步上涨。
Correction Precision: 0.9999999989999999, Recall: 0.0001863585538575873, F1-Score: 0.00037264766126320607
Correction Precision: 0.9812834224572693, Recall: 0.06836810730252078, F1-Score: 0.12783002425992065
Correction Precision: 0.9597834493422429, Recall: 0.46228347923254565, F1-Score: 0.6240100561292341
Correction Precision: 0.9583457304370847, Recall: 0.6002609019752888, F1-Score: 0.7381689006377156
Correction Precision: 0.957552083333084, Recall: 0.6848575153658624, F1-Score: 0.7985666191248599
Correction Precision: 0.956138201420986, Recall: 0.7267647606629303, F1-Score: 0.8258201053292192
Correction Precision: 0.9589596754948797, Recall: 0.7485565282174057, F1-Score: 0.8407949785869138
Correction Precision: 0.9628050225062869, Recall: 0.7569379772768193, F1-Score: 0.8475495302681977
Correction Precision: 0.9666904591955824, Recall: 0.7570337246132369, F1-Score: 0.8491118072397167
Correction Precision: 0.957304408031166, Recall: 0.7727272727271287, F1-Score: 0.8551695696529225
Correction Precision: 0.9608161372592253, Recall: 0.7719821162442675, F1-Score: 0.8561099055072071
Correction Precision: 0.971293001186009, Recall: 0.7635210742258925, F1-Score: 0.8549650198682753
Correction Precision: 0.9754059216807603, Recall: 0.7611328488912313, F1-Score: 0.855049711700003
Correction Precision: 0.971273840357671, Recall: 0.7682994971129132, F1-Score: 0.8579450910207718
Correction Precision: 0.9707822808668777, Recall: 0.7705255283335009, F1-Score: 0.8591387754422067
Correction Precision: 0.9681274900396136, Recall: 0.7699906803353643, F1-Score: 0.8577657802372162
Correction Precision: 0.9760493241638661, Recall: 0.7664804469272315, F1-Score: 0.8586627720114859
Epoch 17 Training: 100% 250/250 [03:51<00:00,  1.08it/s, c_precision=0.995, c_recall=0.98, c_f1_score=0.988, loss=0.000652]
Correction Precision: 0.9683163737278055, Recall: 0.779992548435026, F1-Score: 0.8640115554280332
Eval情况：和PlateauScheduler差不多
Character-level Detect Acc: 0.9847, P: 0.6690, R: 0.5403, F1: 0.5978
Character-level Correct Acc: 0.9836, P: 0.6467, R: 0.4894, F1: 0.5572
Sentence-level Detect Acc: 0.6445, P: 0.7104, R: 0.4771, F1: 0.5708
Sentence-level Correct Acc: 0.6236, P: 0.6910, R: 0.4349, F1: 0.5338



