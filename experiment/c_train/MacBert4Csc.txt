模型：MacBert4Csc，还未使用源代码中的schedule和optimizer
执行情况：效果较差。可以看出schedule和optimizer确实很重要。没有这俩东西在14个epoch就early-stop了
Correction Precision: 0.6147368421051552, Recall: 0.650092764378358, F1-Score: 0.6319206487338203
Correction Precision: 0.6328730212191591, Recall: 0.6963127663515478, F1-Score: 0.663078958477643
Correction Precision: 0.6428099173552656, Recall: 0.719518963922161, F1-Score: 0.6790048008982366
Correction Precision: 0.6482501620219299, Recall: 0.7405145289652525, F1-Score: 0.6913174941025193
Correction Precision: 0.6527597402596342, Recall: 0.7447675495460743, F1-Score: 0.6957349246685837
Correction Precision: 0.6530777976286092, Recall: 0.7450435427087743, F1-Score: 0.6960360043488492
Correction Precision: 0.6545277190699634, Recall: 0.7455555555554174, F1-Score: 0.6970825031813279
Correction Precision: 0.6554379210778544, Recall: 0.7570872707058074, F1-Score: 0.7026051065439953
Correction Precision: 0.657434402332255, Recall: 0.7518058899794865, F1-Score: 0.7014602950175447
Correction Precision: 0.6601159793813369, Recall: 0.7586079229913442, F1-Score: 0.7059431519570685
Correction Precision: 0.658473479948147, Recall: 0.7537948907810526, F1-Score: 0.7029173135018112
Correction Precision: 0.6585562359006351, Recall: 0.7572725588288387, F1-Score: 0.7044729806278802
Correction Precision: 0.660209846650418, Recall: 0.757688032604528, F1-Score: 0.7055982053160944
Correction Precision: 0.6597738287559515, Recall: 0.7564363771067315, F1-Score: 0.704806281321174
Correction Precision: 0.6599001127757919, Recall: 0.758518518518378, F1-Score: 0.7057809937299167

模型：MacBert4Csc，使用了源码中的optimizer（AdamW），但还未使用schedule
执行情况：和不用这个optimizer效果差不多，看来optimizer还是要和schedule配合起来才行，属于联合超参数。
Correction Precision: 0.6342253281062494, Recall: 0.6903525046380908, F1-Score: 0.6610997596500208
Correction Precision: 0.6486311355903696, Recall: 0.7418936446172425, F1-Score: 0.6921348309628036
Correction Precision: 0.6541268039564367, Recall: 0.746345975948058, F1-Score: 0.6972001377668244
Correction Precision: 0.6555466879488179, Recall: 0.7601332593002479, F1-Score: 0.7039766878810791
Correction Precision: 0.6603803739810356, Recall: 0.7653269123910418, F1-Score: 0.7089910770592107
Correction Precision: 0.6593090211131383, Recall: 0.7637576431349334, F1-Score: 0.7077002312821239
Correction Precision: 0.6595471334509941, Recall: 0.7605555555554147, F1-Score: 0.7064591033125052
Correction Precision: 0.6608318612492916, Recall: 0.7624606262737145, F1-Score: 0.7080178935148156
Correction Precision: 0.6632049848217505, Recall: 0.7688460826077479, F1-Score: 0.7121290096244034
Correction Precision: 0.6634553628772216, Recall: 0.7649018881894177, F1-Score: 0.7105760958050654
Correction Precision: 0.6602717825738352, Recall: 0.7645316549424722, F1-Score: 0.7085871145407301
Correction Precision: 0.663052144793388, Recall: 0.7704280155640595, F1-Score: 0.7127185459545171
Correction Precision: 0.6616129032256997, Recall: 0.7610389610388197, F1-Score: 0.7078515957059437
Correction Precision: 0.6592948717947661, Recall: 0.7622753381506833, F1-Score: 0.7070550824276547
Correction Precision: 0.6607228915661588, Recall: 0.7609620721552708, F1-Score: 0.7073086839391652
Correction Precision: 0.6605356566633332, Recall: 0.7577271885987862, F1-Score: 0.7058012235346374
Epoch 16 Training: 100% 250/250 [02:35<00:00,  1.60it/s, loss=0.0109, c_precision=0.732, c_recall=0.997, c_f1_score=0.844]
Correction Precision: 0.6601675797614147, Recall: 0.7588442304128988, F1-Score: 0.706074967188638

模型：MacBert4Csc，完全模拟了MacBert4Csc的训练流程
执行情况：看起来效果并不好，有两种可能性。一是在大数据量上表现会好，二是在我的代码有问题
Correction Precision: 0.5704747774479654, Recall: 0.5706864564006362, F1-Score: 0.5705805967916766
Correction Precision: 0.6170952050033673, Recall: 0.6581434130071042, F1-Score: 0.6369586653302253
Correction Precision: 0.6379776800810554, Recall: 0.6980573543014434, F1-Score: 0.6666666661675599
Correction Precision: 0.6459011060506431, Recall: 0.7349620581157255, F1-Score: 0.6875595181583541
Correction Precision: 0.6516744863289675, Recall: 0.7460640859417028, F1-Score: 0.6956822102102774
Correction Precision: 0.6543369407202948, Recall: 0.7506021863997127, F1-Score: 0.6991715562851054
Correction Precision: 0.6557825317676281, Recall: 0.7549999999998601, F1-Score: 0.7019023839389535
Correction Precision: 0.6571337274323922, Recall: 0.762090050027652, F1-Score: 0.705730953517713
Correction Precision: 0.6591712174749985, Recall: 0.7601407668085274, F1-Score: 0.7060645156314411
Correction Precision: 0.6602564102563044, Recall: 0.7626804887077447, F1-Score: 0.7077821675148379
Correction Precision: 0.6600064453753367, Recall: 0.7582376897443986, F1-Score: 0.7057201924726422
Correction Precision: 0.6613603473226143, Recall: 0.762090050027652, F1-Score: 0.708161156527175
Correction Precision: 0.6618635926992216, Recall: 0.7658391997034519, F1-Score: 0.7100652691693109
Correction Precision: 0.66088353413644, Recall: 0.7619929616594254, F1-Score: 0.7078458357033665
Correction Precision: 0.6600543912972868, Recall: 0.7640740740739325, F1-Score: 0.7082653844480441
Correction Precision: 0.6623272259722497, Recall: 0.763006850583084, F1-Score: 0.7091112444477395
Epoch 16 Training: 100% 250/250 [02:33<00:00,  1.63it/s, loss=0.00943, c_precision=0.728, c_recall=0.997, c_f1_score=0.841]
Correction Precision: 0.6621248986211415, Recall: 0.7550869404364122, F1-Score: 0.7055569954401505
Eval情况：还不错，比我的模型好多了
Character-level Detect Acc: 0.9885, P: 0.7540, R: 0.6733, F1: 0.7114
Character-level Correct Acc: 0.9850, P: 0.6969, R: 0.5050, F1: 0.5856
Sentence-level Detect Acc: 0.7367, P: 0.8014, R: 0.6204, F1: 0.6994
Sentence-level Correct Acc: 0.6664, P: 0.7566, R: 0.4778, F1: 0.5857
一次全量数据Evel情况：经过一次全量数据的训练后，在sighan上的character-level如下，效果还是想当好的。看来optimizer和scheduler非常重要啊。
Sentence Level: acc:0.7482, precision:0.8103, recall:0.6384, f1:0.7141, total num: 1100


模型：MultiModalMacBert4Csc，完全模拟了MacBert4Csc的训练流程，但Bert增加了pinyin和glyph embeddings。因为这样，macbert的预测层初始化没法使用原有的参数。
执行情况：比原本的MacBert4Csc要好。
Correction Precision: 0.002427376586369689, Recall: 0.01708449396471363, F1-Score: 0.004250796806584928
Correction Precision: 0.13678495762710052, Recall: 0.19200743494420222, F1-Score: 0.15975873752830805
Correction Precision: 0.39685501066087503, Recall: 0.276508820798463, F1-Score: 0.32592754685091124
Correction Precision: 0.6617730095989097, Recall: 0.4350408314772763, F1-Score: 0.5249720040005319
Correction Precision: 0.7793904208996663, Recall: 0.5985509938694782, F1-Score: 0.677104128961028
Correction Precision: 0.84168865435336, Recall: 0.6527157738094024, F1-Score: 0.7352540592249704
Correction Precision: 0.8501710376280842, Recall: 0.6930656255808341, F1-Score: 0.7636214661170696
Correction Precision: 0.8714416896233077, Recall: 0.7054450845566427, F1-Score: 0.7797062745387583
Correction Precision: 0.8797297297295315, Recall: 0.7250788936326851, F1-Score: 0.7949526808924939
Correction Precision: 0.8846153846151878, Recall: 0.7375347544020875, F1-Score: 0.8044071560793041
Correction Precision: 0.8910514541385031, Recall: 0.7407476287891499, F1-Score: 0.8089773530127605
Correction Precision: 0.8983430362738695, Recall: 0.7461409708014234, F1-Score: 0.8151986178115439
Correction Precision: 0.8966202783298217, Recall: 0.7546012269937247, F1-Score: 0.8195033308178881
Correction Precision: 0.8888166449933054, Recall: 0.7621259988848239, F1-Score: 0.8206103046553564
Correction Precision: 0.89431248638028, Recall: 0.7625418060199252, F1-Score: 0.8231872425077109
Epoch 16 Training:   1% 3/250 [00:02<03:23,  1.21it/s, loss=0.00413, c_precision=1, c_recall=0.976, c_f1_score=0.988]
Correction Precision: 0.8970331588130678, Recall: 0.7631774313287373, F1-Score: 0.8247091852231009
一次全量数据Eval情况：一次全量eval后，表现不并如原本的MacBert4Csc好，我觉得可能是pinyin和glphy信息确实有效，而且太有效了，导致过拟合了Wang271K数据，
所以在Sighan数据集上表现不好，但在Wang271K的测试集上表现很好。
Character-level Detect Acc: 0.9871, P: 0.7178, R: 0.6250, F1: 0.6682
Character-level Correct Acc: 0.9856, P: 0.6919, R: 0.5517, F1: 0.6139
Sentence-level Detect Acc: 0.7044, P: 0.7761, R: 0.5638, F1: 0.6531
Sentence-level Correct Acc: 0.6734, P: 0.7549, R: 0.5009, F1: 0.6022